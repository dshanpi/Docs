"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[139],{136:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>d,contentTitle:()=>s,default:()=>x,frontMatter:()=>r,metadata:()=>a,toc:()=>p});var i=t(5893),o=t(1151);const r={sidebar_position:3},s="\u4eba\u5f62\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",a={id:"DshanPi-A1/part3/part3-3/03-2-3_HumanShapeDetectionModelDep",title:"\u4eba\u5f62\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",description:"\u53c2\u8003\u8d44\u6599\uff1a",source:"@site/docs/DshanPi-A1/part3/part3-3/03-2-3_HumanShapeDetectionModelDep.md",sourceDirName:"DshanPi-A1/part3/part3-3",slug:"/DshanPi-A1/part3/part3-3/03-2-3_HumanShapeDetectionModelDep",permalink:"/docs/DshanPi-A1/part3/part3-3/03-2-3_HumanShapeDetectionModelDep",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/linuxboard-docs/tree/main/docs/DshanPi-A1/part3/part3-3/03-2-3_HumanShapeDetectionModelDep.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"dshanpia1Sidebar",previous:{title:"\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",permalink:"/docs/DshanPi-A1/part3/part3-3/03-2-2_FaceDetectionModelDep"},next:{title:"\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",permalink:"/docs/DshanPi-A1/part3/part3-3/03-2-4_TargetDetectionModelDep"}},d={},p=[{value:"1.\u83b7\u53d6\u539f\u59cb\u6a21\u578b",id:"1\u83b7\u53d6\u539f\u59cb\u6a21\u578b",level:2},{value:"2.\u6a21\u578b\u8f6c\u6362",id:"2\u6a21\u578b\u8f6c\u6362",level:2},{value:"3.\u6a21\u578b\u63a8\u7406",id:"3\u6a21\u578b\u63a8\u7406",level:2},{value:"4.\u89c6\u9891\u6d41\u63a8\u7406",id:"4\u89c6\u9891\u6d41\u63a8\u7406",level:2}];function l(n){const e={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.a)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"\u4eba\u5f62\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",children:"\u4eba\u5f62\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72"}),"\n",(0,i.jsx)(e.p,{children:"\u53c2\u8003\u8d44\u6599\uff1a"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["\u6a21\u578b\u4ed3\u5e93\uff1a",(0,i.jsx)(e.a,{href:"https://github.com/airockchip/ultralytics_yolov8",children:"https://github.com/airockchip/ultralytics_yolov8"})]}),"\n",(0,i.jsx)(e.li,{}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"1\u83b7\u53d6\u539f\u59cb\u6a21\u578b",children:"1.\u83b7\u53d6\u539f\u59cb\u6a21\u578b"}),"\n",(0,i.jsx)(e.p,{children:"1.\u8fdb\u5165\u4eba\u5f62\u68c0\u6d4b\u76ee\u5f55\uff1a"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"cd ~/Projects/rknn_model_zoo/examples/yolov8_pose/model\n"})}),"\n",(0,i.jsx)(e.p,{children:"2.\u83b7\u53d6\u9884\u8bad\u7ec3\u6a21\u578b"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"chmod +x download_model.sh\n./download_model.sh\n"})}),"\n",(0,i.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"(base) baiwen@dshanpi-a1:~/Projects/rknn_model_zoo/examples/yolov8_pose/model$ ./download_model.sh\n--2025-08-19 14:28:38--  https://ftrg.zbox.filez.com/v2/delivery/data/95f00b0fc900458ba134f8b180b3f7a1/examples/yolov8_pose/yolov8n-pose.onnx\nResolving ftrg.zbox.filez.com (ftrg.zbox.filez.com)... 180.184.171.46\nConnecting to ftrg.zbox.filez.com (ftrg.zbox.filez.com)|180.184.171.46|:443... connected.\nHTTP request sent, awaiting response... 200\nLength: 13326816 (13M) [application/octet-stream]\nSaving to: \u2018./yolov8n-pose.onnx\u2019\n\n./yolov8n-pose.onnx           100%[==============================================>]  12.71M   588KB/s    in 18s\n\n2025-08-19 14:28:56 (731 KB/s) - \u2018./yolov8n-pose.onnx\u2019 saved [13326816/13326816]\n"})}),"\n",(0,i.jsx)(e.h2,{id:"2\u6a21\u578b\u8f6c\u6362",children:"2.\u6a21\u578b\u8f6c\u6362"}),"\n",(0,i.jsxs)(e.p,{children:["1.\u4f7f\u7528Conda\u6fc0\u6d3b",(0,i.jsx)(e.code,{children:"rknn-toolkit2"}),"\u73af\u5883"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"conda activate rknn-toolkit2\n"})}),"\n",(0,i.jsx)(e.p,{children:"2.\u8fdb\u5165yolov8_pose\u6a21\u578b\u8f6c\u6362\u76ee\u5f55"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"cd ~/Projects/rknn_model_zoo/examples/yolov8_pose/python\n"})}),"\n",(0,i.jsx)(e.p,{children:"3.\u6267\u884c\u6a21\u578b\u8f6c\u6362"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"python3 convert.py ../model/yolov8n-pose.onnx rk3576\n"})}),"\n",(0,i.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"(rknn-toolkit2) baiwen@dshanpi-a1:~/Projects/rknn_model_zoo/examples/yolov8_pose/python$ python3 convert.py ../model/yolov8n-pose.onnx rk3576\nI rknn-toolkit2 version: 2.3.2\n--\x3e Config model\ndone\n--\x3e Loading model\nI Loading : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 167/167 [00:00<00:00, 8309.20it/s]\ndone\n--\x3e Building model\nI OpFusing 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00<00:00, 108.73it/s]\nI OpFusing 1 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01<00:00, 52.42it/s]\nI OpFusing 0 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:03<00:00, 26.72it/s]\nI OpFusing 1 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04<00:00, 23.30it/s]\nI OpFusing 0 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04<00:00, 21.51it/s]\nI OpFusing 1 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04<00:00, 20.61it/s]\nI OpFusing 2 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:07<00:00, 14.11it/s]\nI GraphPreparing : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 202/202 [00:00<00:00, 820.24it/s]\nI Quantizating : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 202/202 [01:20<00:00,  2.52it/s]\nW hybrid_quantization_step2: The node that pointed by '/model.22/Slice_3_output_0' is specaifed repeatedly!\nI OpFusing 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00<00:00, 5195.79it/s]\nI OpFusing 1 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00<00:00, 3367.67it/s]\nI OpFusing 2 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00<00:00, 2700.01it/s]\nW hybrid_quantization_step2: The default input dtype of 'images' is changed from 'float32' to 'int8' in rknn model for performance!\n                       Please take care of this change when deploy rknn model with Runtime API!\nW hybrid_quantization_step2: The default output dtype of 'output0' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nI rknn building ...\nI rknn building done.\ndone\n--\x3e Export rknn model\noutput_path: ../model/yolov8_pose.rknn\ndone\n"})}),"\n",(0,i.jsx)(e.p,{children:"\u53ef\u4ee5\u770b\u5230\u8f6c\u6362\u5b8c\u6210\u540e\u5728model\u76ee\u5f55\u4e0b\u770b\u5230\u7aef\u4fa7\u7684RKNN\u6a21\u578b\u3002"}),"\n",(0,i.jsx)(e.h2,{id:"3\u6a21\u578b\u63a8\u7406",children:"3.\u6a21\u578b\u63a8\u7406"}),"\n",(0,i.jsx)(e.p,{children:"\u6267\u884c\u63a8\u7406\u6d4b\u8bd5\u4ee3\u7801\uff1a"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"python3 yolov8_pose.py --model_path ../model/yolov8_pose.rknn --target rk3576\n"})}),"\n",(0,i.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"(rknn-toolkit2) baiwen@dshanpi-a1:~/Projects/rknn_model_zoo/examples/yolov8_pose/python$ python3 yolov8_pose.py --model_path ../model/yolov8_pose.rknn --target rk3576\nI rknn-toolkit2 version: 2.3.2\ndone\n--\x3e Init runtime environment\nI target set by user is: rk3576\ndone\n--\x3e Running model\nW inference: The 'data_format' is not set, and its default value is 'nhwc'!\nsave image in ./result.jpg\n"})}),"\n",(0,i.jsxs)(e.p,{children:["\u8fd0\u884c\u5b8c\u6210\u540e\u53ef\u4ee5\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u751f\u6210",(0,i.jsx)(e.code,{children:"result.jpg"}),"\u7ed3\u679c\u56fe\u50cf\u3002"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.img,{alt:"image-20250819143343991",src:t(4260).Z+"",width:"640",height:"640"})}),"\n",(0,i.jsx)(e.h2,{id:"4\u89c6\u9891\u6d41\u63a8\u7406",children:"4.\u89c6\u9891\u6d41\u63a8\u7406"}),"\n",(0,i.jsxs)(e.blockquote,{children:["\n",(0,i.jsx)(e.p,{children:"\u5f00\u59cb\u524d\u8bf7\u6ce8\u610f\uff0c\u8bf7\u52a1\u5fc5\u63a5\u5165USB\u6444\u50cf\u5934\uff0c\u5e76\u786e\u8ba4/dev/\u76ee\u5f55\u4e0b\u5b58\u5728video0\u8bbe\u5907\u8282\u70b9\uff01\uff01\uff01"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:["1.\u65b0\u5efa\u7a0b\u5e8f\u6587\u4ef6",(0,i.jsx)(e.code,{children:"yolov8_pose_video.py"}),",\u586b\u5165\u4e00\u4e0b\u5185\u5bb9\uff1a"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"import os\nimport sys\nimport urllib\nimport urllib.request\nimport time\nimport numpy as np\nimport argparse\nimport cv2,math\nfrom math import ceil\n\nfrom rknn.api import RKNN\n\nCLASSES = ['person']\n\nnmsThresh = 0.4\nobjectThresh = 0.5\n\ndef letterbox_resize(image, size, bg_color):\n    \"\"\"\n    letterbox_resize the image according to the specified size\n    :param image: input image, which can be a NumPy array or file path\n    :param size: target size (width, height)\n    :param bg_color: background filling data \n    :return: processed image\n    \"\"\"\n    if isinstance(image, str):\n        image = cv2.imread(image)\n\n    target_width, target_height = size\n    image_height, image_width, _ = image.shape\n\n    # Calculate the adjusted image size\n    aspect_ratio = min(target_width / image_width, target_height / image_height)\n    new_width = int(image_width * aspect_ratio)\n    new_height = int(image_height * aspect_ratio)\n\n    # Use cv2.resize() for proportional scaling\n    image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n\n    # Create a new canvas and fill it\n    result_image = np.ones((target_height, target_width, 3), dtype=np.uint8) * bg_color\n    offset_x = (target_width - new_width) // 2\n    offset_y = (target_height - new_height) // 2\n    result_image[offset_y:offset_y + new_height, offset_x:offset_x + new_width] = image\n    return result_image, aspect_ratio, offset_x, offset_y\n\n\nclass DetectBox:\n    def __init__(self, classId, score, xmin, ymin, xmax, ymax, keypoint):\n        self.classId = classId\n        self.score = score\n        self.xmin = xmin\n        self.ymin = ymin\n        self.xmax = xmax\n        self.ymax = ymax\n        self.keypoint = keypoint\n\ndef IOU(xmin1, ymin1, xmax1, ymax1, xmin2, ymin2, xmax2, ymax2):\n    xmin = max(xmin1, xmin2)\n    ymin = max(ymin1, ymin2)\n    xmax = min(xmax1, xmax2)\n    ymax = min(ymax1, ymax2)\n\n    innerWidth = xmax - xmin\n    innerHeight = ymax - ymin\n\n    innerWidth = innerWidth if innerWidth > 0 else 0\n    innerHeight = innerHeight if innerHeight > 0 else 0\n\n    innerArea = innerWidth * innerHeight\n\n    area1 = (xmax1 - xmin1) * (ymax1 - ymin1)\n    area2 = (xmax2 - xmin2) * (ymax2 - ymin2)\n\n    total = area1 + area2 - innerArea\n\n    return innerArea / total\n\n\ndef NMS(detectResult):\n    predBoxs = []\n\n    sort_detectboxs = sorted(detectResult, key=lambda x: x.score, reverse=True)\n\n    for i in range(len(sort_detectboxs)):\n        xmin1 = sort_detectboxs[i].xmin\n        ymin1 = sort_detectboxs[i].ymin\n        xmax1 = sort_detectboxs[i].xmax\n        ymax1 = sort_detectboxs[i].ymax\n        classId = sort_detectboxs[i].classId\n\n        if sort_detectboxs[i].classId != -1:\n            predBoxs.append(sort_detectboxs[i])\n            for j in range(i + 1, len(sort_detectboxs), 1):\n                if classId == sort_detectboxs[j].classId:\n                    xmin2 = sort_detectboxs[j].xmin\n                    ymin2 = sort_detectboxs[j].ymin\n                    xmax2 = sort_detectboxs[j].xmax\n                    ymax2 = sort_detectboxs[j].ymax\n                    iou = IOU(xmin1, ymin1, xmax1, ymax1, xmin2, ymin2, xmax2, ymax2)\n                    if iou > nmsThresh:\n                        sort_detectboxs[j].classId = -1\n    return predBoxs\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef softmax(x, axis=-1):\n    # \u5c06\u8f93\u5165\u5411\u91cf\u51cf\u53bb\u6700\u5927\u503c\u4ee5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\n    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\ndef process(out,keypoints,index,model_w,model_h,stride,scale_w=1,scale_h=1):\n    xywh=out[:,:64,:]\n    conf=sigmoid(out[:,64:,:])\n    out=[]\n    for h in range(model_h):\n        for w in range(model_w):\n            for c in range(len(CLASSES)):\n                if conf[0,c,(h*model_w)+w]>objectThresh:\n                    xywh_=xywh[0,:,(h*model_w)+w] #[1,64,1]\n                    xywh_=xywh_.reshape(1,4,16,1)\n                    data=np.array([i for i in range(16)]).reshape(1,1,16,1)\n                    xywh_=softmax(xywh_,2)\n                    xywh_ = np.multiply(data, xywh_)\n                    xywh_ = np.sum(xywh_, axis=2, keepdims=True).reshape(-1)\n\n                    xywh_temp=xywh_.copy()\n                    xywh_temp[0]=(w+0.5)-xywh_[0]\n                    xywh_temp[1]=(h+0.5)-xywh_[1]\n                    xywh_temp[2]=(w+0.5)+xywh_[2]\n                    xywh_temp[3]=(h+0.5)+xywh_[3]\n\n                    xywh_[0]=((xywh_temp[0]+xywh_temp[2])/2)\n                    xywh_[1]=((xywh_temp[1]+xywh_temp[3])/2)\n                    xywh_[2]=(xywh_temp[2]-xywh_temp[0])\n                    xywh_[3]=(xywh_temp[3]-xywh_temp[1])\n                    xywh_=xywh_*stride\n\n                    xmin=(xywh_[0] - xywh_[2] / 2) * scale_w\n                    ymin = (xywh_[1] - xywh_[3] / 2) * scale_h\n                    xmax = (xywh_[0] + xywh_[2] / 2) * scale_w\n                    ymax = (xywh_[1] + xywh_[3] / 2) * scale_h\n                    keypoint=keypoints[...,(h*model_w)+w+index] \n                    keypoint[...,0:2]=keypoint[...,0:2]//1\n                    box = DetectBox(c,conf[0,c,(h*model_w)+w], xmin, ymin, xmax, ymax,keypoint)\n                    out.append(box)\n\n    return out\n\npose_palette = np.array([[255, 128, 0], [255, 153, 51], [255, 178, 102], [230, 230, 0], [255, 153, 255],\n                         [153, 204, 255], [255, 102, 255], [255, 51, 255], [102, 178, 255], [51, 153, 255],\n                         [255, 153, 153], [255, 102, 102], [255, 51, 51], [153, 255, 153], [102, 255, 102],\n                         [51, 255, 51], [0, 255, 0], [0, 0, 255], [255, 0, 0], [255, 255, 255]],dtype=np.uint8)\nkpt_color  = pose_palette[[16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9]]\nskeleton = [[16, 14], [14, 12], [17, 15], [15, 13], [12, 13], [6, 12], [7, 13], [6, 7], [6, 8], \n            [7, 9], [8, 10], [9, 11], [2, 3], [1, 2], [1, 3], [2, 4], [3, 5], [4, 6], [5, 7]]\nlimb_color = pose_palette[[9, 9, 9, 9, 7, 7, 7, 0, 0, 0, 0, 0, 16, 16, 16, 16, 16, 16, 16]]\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='YOLOv8-Pose Real-time Demo', add_help=True)\n    parser.add_argument('--model_path', type=str, required=True,\n                        help='model path, could be .rknn file')\n    parser.add_argument('--target', type=str,\n                        default='rk3566', help='target RKNPU platform')\n    parser.add_argument('--device_id', type=str,\n                        default=None, help='device id')\n    args = parser.parse_args()\n\n    # 1. \u52a0\u8f7d RKNN \u6a21\u578b\n    rknn = RKNN(verbose=False)\n    ret = rknn.load_rknn(args.model_path)\n    if ret != 0:\n        print('Load RKNN model failed!')\n        exit(ret)\n    ret = rknn.init_runtime(target=args.target, device_id=args.device_id)\n    if ret != 0:\n        print('Init runtime failed!')\n        exit(ret)\n    print('Model & runtime ready.')\n\n    # 2. \u6253\u5f00\u6444\u50cf\u5934\n    cap = cv2.VideoCapture(0)\n    if not cap.isOpened():\n        print('Cannot open camera.')\n        exit(-1)\n\n    model_w, model_h = 640, 640\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # 3. \u9884\u5904\u7406\n        letterbox_img, ar, off_x, off_y = letterbox_resize(\n            frame, (model_w, model_h), 56)\n        infer_img = letterbox_img[..., ::-1]      # BGR\u2192RGB\n        infer_img = infer_img.astype(np.float32)\n\n        # 4. RKNN \u63a8\u7406\n        results = rknn.inference(inputs=[infer_img])\n\n        # 5. \u540e\u5904\u7406\n        outputs = []\n        keypoints = results[3]\n        for x in results[:3]:\n            idx, stride = 0, 0\n            if x.shape[2] == 20:\n                stride, idx = 32, 20*4*20*4 + 20*2*20*2\n            elif x.shape[2] == 40:\n                stride, idx = 16, 20*4*20*4\n            elif x.shape[2] == 80:\n                stride, idx = 8, 0\n            feature = x.reshape(1, 65, -1)\n            outputs += process(feature, keypoints, idx,\n                               x.shape[3], x.shape[2], stride)\n        predbox = NMS(outputs)\n\n        # 6. \u753b\u6846\u3001\u753b\u5173\u952e\u70b9\n        for box in predbox:\n            xmin = int((box.xmin - off_x) / ar)\n            ymin = int((box.ymin - off_y) / ar)\n            xmax = int((box.xmax - off_x) / ar)\n            ymax = int((box.ymax - off_y) / ar)\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f\"{CLASSES[box.classId]}:{box.score:.2f}\",\n                        (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX,\n                        0.7, (0, 0, 255), 2, cv2.LINE_AA)\n\n            kpts = box.keypoint.reshape(-1, 3)\n            kpts[..., 0] = (kpts[..., 0] - off_x) / ar\n            kpts[..., 1] = (kpts[..., 1] - off_y) / ar\n\n            # \u753b\u70b9\n            for k, (x, y, conf) in enumerate(kpts):\n                if x != 0 and y != 0:\n                    cv2.circle(frame, (int(x), int(y)), 5,\n                               [int(c) for c in kpt_color[k]], -1)\n\n            # \u753b\u9aa8\u67b6\n            for k, sk in enumerate(skeleton):\n                pos1 = (int(kpts[sk[0]-1, 0]), int(kpts[sk[0]-1, 1]))\n                pos2 = (int(kpts[sk[1]-1, 0]), int(kpts[sk[1]-1, 1]))\n                if 0 in pos1 + pos2:\n                    continue\n                cv2.line(frame, pos1, pos2,\n                         [int(c) for c in limb_color[k]], 2, cv2.LINE_AA)\n\n        # 7. \u5b9e\u65f6\u663e\u793a\n        cv2.imshow('YOLOv8-Pose', frame)\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n    rknn.release()\n"})}),"\n",(0,i.jsx)(e.p,{children:"2.\u8fd0\u884c\u89c6\u9891\u6d41\u63a8\u7406"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"python3 yolov8_pose_video.py --model_path ../model/yolov8_pose.rknn --target rk3576\n"})})]})}function x(n={}){const{wrapper:e}={...(0,o.a)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(l,{...n})}):l(n)}},4260:(n,e,t)=>{t.d(e,{Z:()=>i});const i=t.p+"assets/images/image-20250819143343991-045bcef3cd8ff3323f490f4e0311abbb.png"},1151:(n,e,t)=>{t.d(e,{Z:()=>a,a:()=>s});var i=t(7294);const o={},r=i.createContext(o);function s(n){const e=i.useContext(r);return i.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),i.createElement(r.Provider,{value:e},n.children)}}}]);