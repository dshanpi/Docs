"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[615],{8224:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>o,contentTitle:()=>t,default:()=>p,frontMatter:()=>s,metadata:()=>c,toc:()=>l});var r=i(5893),a=i(1151);const s={sidebar_position:1},t="OpenCV",c={id:"DshanPi-A1/part3/part3-1/03-1-1_OpenCVApplications",title:"OpenCV",description:"\u200b\tOpenCV\uff08Open Source Computer Vision Library\uff0c\u5f00\u6e90\u8ba1\u7b97\u673a\u89c6\u89c9\u5e93\uff09\u8bde\u751f\u4e8e 1999 \u5e74 Intel Research\uff0c\u5982\u4eca\u7531 OpenCV.org \u57fa\u91d1\u4f1a\u7ef4\u62a4\uff0c\u662f\u76ee\u524d\u5168\u7403\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u5f00\u6e90\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u673a\u5668\u5b66\u4e60\u8f6f\u4ef6\u5e93\u4e4b\u4e00\u3002",source:"@site/docs/DshanPi-A1/part3/part3-1/03-1-1_OpenCVApplications.md",sourceDirName:"DshanPi-A1/part3/part3-1",slug:"/DshanPi-A1/part3/part3-1/03-1-1_OpenCVApplications",permalink:"/docs/DshanPi-A1/part3/part3-1/03-1-1_OpenCVApplications",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/linuxboard-docs/tree/main/docs/DshanPi-A1/part3/part3-1/03-1-1_OpenCVApplications.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"dshanpia1Sidebar",previous:{title:"\u6444\u50cf\u5934\u4e0e\u663e\u793a\u5e94\u7528",permalink:"/docs/category/\u6444\u50cf\u5934\u4e0e\u663e\u793a\u5e94\u7528"},next:{title:"\u591a\u5a92\u4f53\u5e94\u7528",permalink:"/docs/category/\u591a\u5a92\u4f53\u5e94\u7528"}},o={},l=[{value:"1.Python\u5e93",id:"1python\u5e93",level:2},{value:"1.1 APT\u5b89\u88c5",id:"11-apt\u5b89\u88c5",level:3},{value:"1.2 \u6d4b\u8bd5",id:"12-\u6d4b\u8bd5",level:3},{value:"2.C++\u5e93",id:"2c\u5e93",level:2},{value:"2.1 APT\u5b89\u88c5",id:"21-apt\u5b89\u88c5",level:3},{value:"2.2 \u6d4b\u8bd5",id:"22-\u6d4b\u8bd5",level:3},{value:"3.\u672c\u5730\u7f16\u8bd1",id:"3\u672c\u5730\u7f16\u8bd1",level:2},{value:"4.\u66f4\u591a\u793a\u4f8b\u4ee3\u7801",id:"4\u66f4\u591a\u793a\u4f8b\u4ee3\u7801",level:2},{value:"4.1 Canny \u8fb9\u7f18\u68c0\u6d4b",id:"41-canny-\u8fb9\u7f18\u68c0\u6d4b",level:3},{value:"4.2 \u7ebf\u6bb5\u68c0\u6d4b",id:"42-\u7ebf\u6bb5\u68c0\u6d4b",level:3},{value:"4.3 \u5706\u68c0\u6d4b",id:"43-\u5706\u68c0\u6d4b",level:3},{value:"4.4 \u77e9\u5f62\u68c0\u6d4b",id:"44-\u77e9\u5f62\u68c0\u6d4b",level:3},{value:"4.5 \u5feb\u901f\u7ebf\u6027\u56de\u5f52",id:"45-\u5feb\u901f\u7ebf\u6027\u56de\u5f52",level:3},{value:"4.6 \u89d2\u70b9\u68c0\u6d4b",id:"46-\u89d2\u70b9\u68c0\u6d4b",level:3},{value:"4.7 \u4eba\u8138\u68c0\u6d4b",id:"47-\u4eba\u8138\u68c0\u6d4b",level:3}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.a)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.h1,{id:"opencv",children:"OpenCV"}),"\n",(0,r.jsxs)(e.p,{children:["\u200b\tOpenCV\uff08Open Source Computer Vision Library\uff0c\u5f00\u6e90\u8ba1\u7b97\u673a\u89c6\u89c9\u5e93\uff09\u8bde\u751f\u4e8e 1999 \u5e74 Intel Research\uff0c\u5982\u4eca\u7531 OpenCV.org \u57fa\u91d1\u4f1a\u7ef4\u62a4\uff0c\u662f\u76ee\u524d",(0,r.jsx)(e.strong,{children:"\u5168\u7403\u4f7f\u7528\u6700\u5e7f\u6cdb"}),"\u7684\u5f00\u6e90\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u673a\u5668\u5b66\u4e60\u8f6f\u4ef6\u5e93\u4e4b\u4e00\u3002"]}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"\u7ef4\u5ea6"}),(0,r.jsx)(e.th,{children:"\u8bf4\u660e"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"\u8bed\u8a00\u7ed1\u5b9a"})}),(0,r.jsx)(e.td,{children:"C++\uff08\u539f\u751f\uff09\u3001Python\uff08\u6700\u6d41\u884c\uff09\u3001Java\u3001JavaScript\u3001MATLAB\u3001Go\u3001Rust \u7b49"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"\u5e73\u53f0\u8986\u76d6"})}),(0,r.jsx)(e.td,{children:"Linux\u3001Windows\u3001macOS\u3001Android\u3001iOS\u3001RTOS\u3001WebAssembly"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"\u786c\u4ef6\u52a0\u901f"})}),(0,r.jsx)(e.td,{children:"CPU\uff08SSE/AVX/NEON\uff09\u3001CUDA\u3001OpenCL\u3001Vulkan\u3001Metal\u3001NPU\uff08VPI\u3001RKNPU\u3001TIM-VX\u2026\uff09"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"\u7b97\u6cd5\u89c4\u6a21"})}),(0,r.jsx)(e.td,{children:"2500+ \u4f18\u5316\u7b97\u6cd5\uff0c\u6db5\u76d6\u4f20\u7edf CV \u4e0e\u6df1\u5ea6\u5b66\u4e60"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"\u751f\u6001\u5de5\u5177"})}),(0,r.jsx)(e.td,{children:"\u8bad\u7ec3\u6846\u67b6\uff08Model Zoo\uff09\u3001\u6807\u6ce8\u5de5\u5177\uff08CVAT\uff09\u3001\u4f18\u5316\u5de5\u5177\uff08OpenVINO\u3001TensorRT \u63d2\u4ef6\uff09"})]})]})]}),"\n",(0,r.jsx)(e.h2,{id:"1python\u5e93",children:"1.Python\u5e93"}),"\n",(0,r.jsx)(e.h3,{id:"11-apt\u5b89\u88c5",children:"1.1 APT\u5b89\u88c5"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"pip3 install opencv-python\n"})}),"\n",(0,r.jsx)(e.p,{children:"\u5982\u679c\u662f\u56fd\u5185\u7528\u6237\u53ef\u6307\u5b9a\u56fd\u5185\u6e90\u5b89\u88c5\uff0c\u6267\u884c\uff1a"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"pip3 install opencv-python -i https://pypi.tuna.tsinghua.edu.cn/simple\n"})}),"\n",(0,r.jsx)(e.h3,{id:"12-\u6d4b\u8bd5",children:"1.2 \u6d4b\u8bd5"}),"\n",(0,r.jsxs)(e.p,{children:["\u65b0\u5efa\u6587\u4ef6",(0,r.jsx)(e.code,{children:"test.py"}),"\uff0c\u5c06\u4e0b\u9762\u7684\u5185\u5bb9\u586b\u5165\uff1a"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:'import cv2, sys, numpy as np\nprint("OpenCV Version:", cv2.__version__)\nprint("Python Version:", sys.version.split()[0])\n\nimg = np.full((400,400,3), (255,0,0), dtype=np.uint8)  # BGR Blue\ncv2.imshow("OpenCV Test", img)\ncv2.waitKey(3000)\ncv2.destroyAllWindows()\nprint("OpenCV Ready\uff01")\n'})}),"\n",(0,r.jsx)(e.p,{children:"\u586b\u5199\u5b8c\u4ee3\u7801\u540e\u6267\u884c\uff1a"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"python3 test.py\n"})}),"\n",(0,r.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"image-20250813151449466",src:i(8974).Z+"",width:"1143",height:"585"})}),"\n",(0,r.jsx)(e.h2,{id:"2c\u5e93",children:"2.C++\u5e93"}),"\n",(0,r.jsx)(e.h3,{id:"21-apt\u5b89\u88c5",children:"2.1 APT\u5b89\u88c5"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"sudo apt install libopencv-dev -y\n"})}),"\n",(0,r.jsx)(e.h3,{id:"22-\u6d4b\u8bd5",children:"2.2 \u6d4b\u8bd5"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:'#include <opencv2/opencv.hpp>\n\nint main()\n{\n    // \u753b\u5e03\u5c3a\u5bf8\n    int width = 400, height = 200;\n\n    // \u521b\u5efa\u84dd\u8272\u80cc\u666f (BGR)\n    cv::Mat img(height, width, CV_8UC3, cv::Scalar(255, 0, 0));\n\n    // \u914d\u7f6e\u6587\u5b57\n    std::string text = "Hello OpenCV";\n    int fontFace = cv::FONT_HERSHEY_SIMPLEX;\n    double fontScale = 1.0;\n    int thickness = 2;\n\n    // \u8ba1\u7b97\u6587\u5b57\u5927\u5c0f\uff0c\u4f7f\u6587\u5b57\u5c45\u4e2d\n    int baseline = 0;\n    cv::Size textSize = cv::getTextSize(text, fontFace, fontScale, thickness, &baseline);\n    cv::Point org((width - textSize.width) / 2,\n                  (height + textSize.height) / 2);\n\n    // \u7ed8\u5236\u6587\u5b57\uff08\u767d\u8272\uff09\n    cv::putText(img, text, org, fontFace, fontScale,\n                cv::Scalar(255, 255, 255), thickness, cv::LINE_AA);\n\n    // \u4fdd\u5b58\u5230\u78c1\u76d8\n    cv::imwrite("hello_opencv.jpg", img);\n\n    cv::imshow("Hello", img);\n    cv::waitKey(3000);\n\n    return 0;\n}\n'})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u7f16\u8bd1 & \u8fd0\u884c:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"# \u7f16\u8bd1\ng++ hello_opencv.cpp -o hello_opencv `pkg-config --cflags --libs opencv4`\n\n# \u8fd0\u884c\n./hello_opencv\n"})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"image-20250813152710416",src:i(5510).Z+"",width:"745",height:"852"})}),"\n",(0,r.jsx)(e.h2,{id:"3\u672c\u5730\u7f16\u8bd1",children:"3.\u672c\u5730\u7f16\u8bd1"}),"\n",(0,r.jsx)(e.p,{children:"1.\u4e0b\u8f7dOpenCV\u5305\uff08\u4ee54.12\u4e3a\u4f8b\uff09"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"wget https://github.com/opencv/opencv/archive/refs/tags/4.12.0.zip\n"})}),"\n",(0,r.jsxs)(e.p,{children:["\u5982\u679c\u65e0\u6cd5\u4e0b\u8f7d\u53ef\u8bbf\u95eeOpenCV\u5b98\u7f51\u8d44\u6e90\u4e0b\u8f7d\uff1a",(0,r.jsx)(e.a,{href:"https://opencv.org/releases/",children:"https://opencv.org/releases/"})]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"image-20250813161203024",src:i(8886).Z+"",width:"868",height:"351"})}),"\n",(0,r.jsx)(e.p,{children:"2.\u89e3\u538bOpenCV\u5e93"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"unzip 4.12.0.zip\n"})}),"\n",(0,r.jsx)(e.p,{children:"\u89e3\u538b\u5b8c\u6210\u540e\u8fdb\u5165OpenCV\u5e93\u76ee\u5f55\u3002"}),"\n",(0,r.jsx)(e.p,{children:"3.\u5b89\u88c5\u7f16\u8bd1\u4f9d\u8d56"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"sudo apt update\nsudo apt install -y build-essential cmake git pkg-config \\\n  libgtk-3-dev libavcodec-dev libavformat-dev libswscale-dev \\\n  libtbb-dev libjpeg-dev libpng-dev libtiff-dev libdc1394-dev \\\n  libv4l-dev libxvidcore-dev libx264-dev libopenexr-dev \\\n  libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev \\\n  python3-dev python3-numpy\n"})}),"\n",(0,r.jsx)(e.p,{children:"4.\u7f16\u8bd1"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"\u5728OpenCV\u5e93\u76ee\u5f55\u4e0b\u65b0\u5efa\u7f16\u8bd1\u6587\u4ef6\u5939\uff1a"}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"mkdir build\ncd build\n"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"\u751f\u6210cmake\u7f16\u8bd1\u89c4\u5219"}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"cmake ..\n"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"\u7f16\u8bd1"}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"make -j$(nproc)\n"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"\u5b89\u88c5"}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"sudo make install\nsudo ldconfig\n"})}),"\n",(0,r.jsx)(e.h2,{id:"4\u66f4\u591a\u793a\u4f8b\u4ee3\u7801",children:"4.\u66f4\u591a\u793a\u4f8b\u4ee3\u7801"}),"\n",(0,r.jsx)(e.p,{children:"\u6ce8\u610f\uff1a\u4e0b\u9762\u7684Python\u4ee3\u7801\u5747\u4f1a\u4f7f\u7528USB\u6444\u50cf\u5934\uff0c\u8bf7\u63d0\u524d\u5c06USB\u6444\u50cf\u5934\u63a5\u5165\u5230\u677f\u5b50\u7684USB\u53e3\u4e2d\uff01\uff01\uff01"}),"\n",(0,r.jsx)(e.h3,{id:"41-canny-\u8fb9\u7f18\u68c0\u6d4b",children:"4.1 Canny \u8fb9\u7f18\u68c0\u6d4b"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:'#!/usr/bin/env python3\nimport cv2\n\n# Open the default USB camera (index 0)\ncap = cv2.VideoCapture(0, cv2.CAP_V4L2)  # CAP_V4L2 is faster on Linux\nif not cap.isOpened():\n    raise RuntimeError("Cannot open camera; check connection or permissions")\n\n# Optional: set resolution\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n\n# Canny thresholds\nlow_th  = 100\nhigh_th = 200\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        print("Failed to grab frame")\n        break\n\n    # 1. Convert to grayscale and apply Gaussian blur\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n\n    # 2. Canny edge detection\n    edges = cv2.Canny(blur, low_th, high_th)\n\n    # 3. Convert single-channel edge map back to 3-channel for display\n    edges_bgr = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n\n    # 4. Concatenate original and edge images side by side\n    combined = cv2.hconcat([frame, edges_bgr])\n    cv2.imshow("USB Canny (left: original, right: edges)", combined)\n\n    # 5. Exit on \'q\' or ESC\n    key = cv2.waitKey(1) & 0xFF\n    if key in (27, ord(\'q\')):\n        break\n\n# Release resources\ncap.release()\ncv2.destroyAllWindows()\n'})}),"\n",(0,r.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\uff1a"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"image-20250813165936033",src:i(9591).Z+"",width:"1273",height:"552"})}),"\n",(0,r.jsx)(e.h3,{id:"42-\u7ebf\u6bb5\u68c0\u6d4b",children:"4.2 \u7ebf\u6bb5\u68c0\u6d4b"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:'#!/usr/bin/env python3\nimport cv2\nimport numpy as np\n\n# open default USB camera\ncap = cv2.VideoCapture(0, cv2.CAP_V4L2)\nif not cap.isOpened():\n    raise RuntimeError("Cannot open camera")\n\n# optional resolution\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n\n# Hough parameters\nrho_res   = 1          # pixel resolution\ntheta_res = np.pi/180  # radian resolution\nthresh    = 80         # accumulator threshold\nmin_len   = 50         # minimum line length\nmax_gap   = 10         # maximum gap between segments\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        print("Failed to grab frame")\n        break\n\n    # 1. grayscale + blur\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n\n    # 2. Canny edge map\n    edges = cv2.Canny(blur, 50, 150)\n\n    # 3. Probabilistic Hough Transform\n    lines = cv2.HoughLinesP(edges,\n                            rho_res,\n                            theta_res,\n                            threshold=thresh,\n                            minLineLength=min_len,\n                            maxLineGap=max_gap)\n\n    # 4. draw lines on copy of original frame\n    display = frame.copy()\n    if lines is not None:\n        for x1, y1, x2, y2 in lines[:, 0]:\n            cv2.line(display, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n    # 5. show result\n    cv2.imshow("Hough Line Detection (USB)", display)\n\n    # 6. exit on \'q\' or ESC\n    key = cv2.waitKey(1) & 0xFF\n    if key in (27, ord(\'q\')):\n        break\n\n# cleanup\ncap.release()\ncv2.destroyAllWindows()\n'})}),"\n",(0,r.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\uff1a"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"image-20250813170342488",src:i(4124).Z+"",width:"625",height:"570"})}),"\n",(0,r.jsx)(e.h3,{id:"43-\u5706\u68c0\u6d4b",children:"4.3 \u5706\u68c0\u6d4b"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:'#!/usr/bin/env python3\nimport cv2\nimport numpy as np\n\n# camera configuration\nSENSOR_W, SENSOR_H = 1280,960   # native sensor resolution\nFRAME_W, FRAME_H   = 320, 240    # processing / display resolution\n\n# open camera 0 and set native resolution\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH,  SENSOR_W)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, SENSOR_H)\n\nif not cap.isOpened():\n    print(f"Failed to open camera 1 at {SENSOR_W}x{SENSOR_H}! Check connection.")\n    exit()\n\nprint(f"Camera configured: capture {SENSOR_W}x{SENSOR_H} \u2192 process {FRAME_W}x{FRAME_H}")\n\n# scale factor for parameter adaptation\nSCALE_FACTOR = max(SENSOR_W / FRAME_W, SENSOR_H / FRAME_H)\n\nwhile True:\n    # grab full-resolution frame\n    ret, frame_high_res = cap.read()\n    if not ret:\n        print("Frame capture failed, exiting...")\n        break\n\n    # down-scale for processing\n    frame = cv2.resize(frame_high_res, (FRAME_W, FRAME_H))\n\n    # grayscale + blur\n    gray  = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    blur  = cv2.GaussianBlur(gray, (9, 9), 2)\n\n    # adapt detection parameters according to scale\n    min_dist_scaled   = max(20, int(50  / SCALE_FACTOR))\n    min_radius_scaled = max(5,  int(10  / SCALE_FACTOR))\n    max_radius_scaled = min(100,int(100 / SCALE_FACTOR))\n\n    # detect circles via Hough transform\n    circles = cv2.HoughCircles(\n        blur,\n        cv2.HOUGH_GRADIENT,\n        dp=1,\n        minDist=min_dist_scaled,\n        param1=50,\n        param2=30,\n        minRadius=min_radius_scaled,\n        maxRadius=max_radius_scaled\n    )\n\n    # draw detected circles on the high-resolution frame\n    if circles is not None:\n        circles = np.uint16(np.around(circles))\n        for x, y, r in circles[0, :]:\n            # map coordinates back to high-resolution space\n            x_hr = int(x * SCALE_FACTOR)\n            y_hr = int(y * SCALE_FACTOR)\n            r_hr = int(r * SCALE_FACTOR)\n\n            # draw circle and center\n            cv2.circle(frame_high_res, (x_hr, y_hr), r_hr, (0, 255, 0), 3)\n            cv2.circle(frame_high_res, (x_hr, y_hr), 3,   (0, 0, 255), 5)\n\n    # overlay resolution info\n    cv2.putText(frame_high_res,\n                f"Sensor: {SENSOR_W}x{SENSOR_H} | Display: {FRAME_W}x{FRAME_H}",\n                (10, 30),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.7,\n                (0, 0, 255),\n                2)\n\n    # display result\n    cv2.imshow("Real-time Circle Detection", frame_high_res)\n\n    # quit on \'q\'\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n        break\n\n# cleanup\ncap.release()\ncv2.destroyAllWindows()\n'})}),"\n",(0,r.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\uff1a"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"image-20250813171909887",src:i(5358).Z+"",width:"1275",height:"890"})}),"\n",(0,r.jsx)(e.h3,{id:"44-\u77e9\u5f62\u68c0\u6d4b",children:"4.4 \u77e9\u5f62\u68c0\u6d4b"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:'#!/usr/bin/env python3\nimport cv2\nimport numpy as np\n\n# ------------ camera setup ------------\ncap = cv2.VideoCapture(0, cv2.CAP_V4L2)  # change index if necessary\nif not cap.isOpened():\n    raise RuntimeError("Cannot open camera")\n\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n\n# ------------ parameters ------------\n# Canny thresholds\nCANNY_LOW  = 50\nCANNY_HIGH = 150\n# contour approximation accuracy (epsilon = arcLength * ratio)\nAPPROX_RATIO = 0.02\n# min area filter (ignore tiny noise)\nMIN_AREA = 500\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        print("Frame read failed")\n        break\n\n    # ---------- pre-processing ----------\n    gray  = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    blur  = cv2.GaussianBlur(gray, (5, 5), 0)\n    edges = cv2.Canny(blur, CANNY_LOW, CANNY_HIGH)\n\n    # ---------- find contours ----------\n    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL,\n                                   cv2.CHAIN_APPROX_SIMPLE)\n\n    # ---------- loop over contours ----------\n    for cnt in contours:\n        area = cv2.contourArea(cnt)\n        if area < MIN_AREA:\n            continue\n\n        # polygonal approximation\n        epsilon = APPROX_RATIO * cv2.arcLength(cnt, True)\n        approx  = cv2.approxPolyDP(cnt, epsilon, True)\n\n        # we need 4 vertices => rectangle\n        if len(approx) == 4:\n            cv2.drawContours(frame, [approx], -1, (0, 255, 0), 2)\n\n    # ---------- display ----------\n    cv2.imshow("Real-time Rectangle Detection", frame)\n\n    # quit on \'q\' or ESC\n    key = cv2.waitKey(1) & 0xFF\n    if key in (27, ord(\'q\')):\n        break\n\n# ---------- cleanup ----------\ncap.release()\ncv2.destroyAllWindows()\n'})}),"\n",(0,r.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\uff1a"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"image-20250813172318211",src:i(7017).Z+"",width:"632",height:"563"})}),"\n",(0,r.jsx)(e.h3,{id:"45-\u5feb\u901f\u7ebf\u6027\u56de\u5f52",children:"4.5 \u5feb\u901f\u7ebf\u6027\u56de\u5f52"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:'#!/usr/bin/env python3\nimport cv2\nimport numpy as np\n\n# ---------- camera ----------\ncap = cv2.VideoCapture(0, cv2.CAP_V4L2)\nif not cap.isOpened():\n    raise RuntimeError("Cannot open camera")\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n\n# ---------- parameters ----------\nCANNY_LOW   = 50\nCANNY_HIGH  = 150\nHOUGH_RHO   = 1\nHOUGH_THETA = np.pi / 180\nHOUGH_TH    = 100            # Hough threshold\nMIN_LINE    = 50             # minLineLength\nMAX_GAP     = 10             # maxLineGap\nDRAW_COLOR  = (0, 255, 0)\nDRAW_THICK  = 3\n\ndef draw_regression(frame, points):\n    """Fit y = kx + b with NumPy polyfit and draw the line."""\n    if points.shape[0] < 2:\n        return\n    x, y = points[:, 0], points[:, 1]\n    k, b = np.polyfit(x, y, 1)          # 1-degree polynomial = linear\n    h, w = frame.shape[:2]\n    x0, x1 = 0, w\n    y0, y1 = int(k * x0 + b), int(k * x1 + b)\n    cv2.line(frame, (x0, y0), (x1, y1), DRAW_COLOR, DRAW_THICK)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        print("Frame read failed")\n        break\n\n    gray  = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    blur  = cv2.GaussianBlur(gray, (5, 5), 0)\n    edges = cv2.Canny(blur, CANNY_LOW, CANNY_HIGH)\n\n    # Hough line segments\n    lines = cv2.HoughLinesP(edges,\n                            HOUGH_RHO,\n                            HOUGH_THETA,\n                            HOUGH_TH,\n                            minLineLength=MIN_LINE,\n                            maxLineGap=MAX_GAP)\n\n    # collect endpoints\n    pts = []\n    if lines is not None:\n        for x1, y1, x2, y2 in lines[:, 0]:\n            pts.extend([[x1, y1], [x2, y2]])\n\n    if pts:\n        pts = np.array(pts, dtype=np.float32)\n        draw_regression(frame, pts)\n\n    cv2.imshow("USB Line Regression", frame)\n    if cv2.waitKey(1) & 0xFF in (27, ord(\'q\')):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n'})}),"\n",(0,r.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\uff1a"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"image-20250813172747657",src:i(7184).Z+"",width:"627",height:"571"})}),"\n",(0,r.jsx)(e.h3,{id:"46-\u89d2\u70b9\u68c0\u6d4b",children:"4.6 \u89d2\u70b9\u68c0\u6d4b"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:'#!/usr/bin/env python3\nimport cv2\nimport numpy as np\n\n# ---------- open camera ----------\ncap = cv2.VideoCapture(0, cv2.CAP_V4L2)   # change index if needed\nif not cap.isOpened():\n    raise RuntimeError("Cannot open camera")\n\n# optional resolution\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n\n# ---------- Harris parameters ----------\nHARRIS_BLOCK_SIZE = 2          # neighbourhood size\nHARRIS_KSIZE      = 3          # Sobel aperture\nHARRIS_K          = 0.04       # Harris detector free parameter\nTHRESH_RATIO      = 0.01       # corner response threshold ratio\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        print("Frame read failed")\n        break\n\n    # ---------- Harris corner detection ----------\n    gray  = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    gray  = np.float32(gray)\n    dst   = cv2.cornerHarris(gray,\n                             blockSize=HARRIS_BLOCK_SIZE,\n                             ksize=HARRIS_KSIZE,\n                             k=HARRIS_K)\n    # dilate corners for better visibility\n    dst = cv2.dilate(dst, None)\n\n    # threshold corner response\n    frame[dst > THRESH_RATIO * dst.max()] = [0, 0, 255]\n\n    # ---------- show ----------\n    cv2.imshow("Harris Corner Detection (USB)", frame)\n\n    # quit on \'q\' or ESC\n    key = cv2.waitKey(1) & 0xFF\n    if key in (27, ord(\'q\')):\n        break\n\n# ---------- cleanup ----------\ncap.release()\ncv2.destroyAllWindows()\n'})}),"\n",(0,r.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\uff1a"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"image-20250813173720307",src:i(5920).Z+"",width:"624",height:"549"})}),"\n",(0,r.jsx)(e.h3,{id:"47-\u4eba\u8138\u68c0\u6d4b",children:"4.7 \u4eba\u8138\u68c0\u6d4b"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"#!/usr/bin/env python3\nimport cv2\n\n# 0. \u6253\u5f00\u6444\u50cf\u5934\ncap = cv2.VideoCapture(0, cv2.CAP_V4L2)\nif not cap.isOpened():\n    raise RuntimeError(\"Cannot open camera\")\n\n# 1. \u52a0\u8f7d Haar \u4eba\u8138\u6a21\u578b\uff08\u5df2\u968f OpenCV \u5b89\u88c5\uff09\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades +\n                                     'haarcascade_frontalface_default.xml')\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # 2. \u7070\u5ea6\u5316 + \u68c0\u6d4b\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(gray,\n                                          scaleFactor=1.1,\n                                          minNeighbors=5,\n                                          minSize=(50, 50))\n\n    # 3. \u753b\u6846\n    for (x, y, w, h) in faces:\n        cv2.rectangle(frame, (x, y), (x + w, y + h),\n                      (0, 255, 0), 2)\n\n    # 4. \u663e\u793a\n    cv2.imshow(\"USB Face Detection\", frame)\n    if cv2.waitKey(1) & 0xFF in (27, ord('q')):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n"})}),"\n",(0,r.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\uff1a"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"image-20250813174248289",src:i(681).Z+"",width:"632",height:"571"})})]})}function p(n={}){const{wrapper:e}={...(0,a.a)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8974:(n,e,i)=>{i.d(e,{Z:()=>r});const r=i.p+"assets/images/image-20250813151449466-035c508f65e3a57ebc5d3f294dc368d5.png"},5510:(n,e,i)=>{i.d(e,{Z:()=>r});const r=i.p+"assets/images/image-20250813152710416-6316cb1536da570f274a00f1f9ad3966.png"},8886:(n,e,i)=>{i.d(e,{Z:()=>r});const r=i.p+"assets/images/image-20250813161203024-695c4382679a92fdcb8787b8633bd7af.png"},9591:(n,e,i)=>{i.d(e,{Z:()=>r});const r=i.p+"assets/images/image-20250813165936033-9c127007ef282cbac03d03650cdc87b9.png"},4124:(n,e,i)=>{i.d(e,{Z:()=>r});const r=i.p+"assets/images/image-20250813170342488-12c3bd70dabc4aa6efe9ad4f6d15b659.png"},5358:(n,e,i)=>{i.d(e,{Z:()=>r});const r=i.p+"assets/images/image-20250813171909887-4989214c32ef7b96594d3d1eb102ba25.png"},7017:(n,e,i)=>{i.d(e,{Z:()=>r});const r=i.p+"assets/images/image-20250813172318211-9599e48c544d5b57d9e124e1cee6fd65.png"},7184:(n,e,i)=>{i.d(e,{Z:()=>r});const r=i.p+"assets/images/image-20250813172747657-b5739d2a0e0335a7e6037078df957efc.png"},5920:(n,e,i)=>{i.d(e,{Z:()=>r});const r=i.p+"assets/images/image-20250813173720307-ba5ded67a45bdd054f04c321be67c873.png"},681:(n,e,i)=>{i.d(e,{Z:()=>r});const r=i.p+"assets/images/image-20250813174248289-528e309636daac582ff49e9a71d5ef2e.png"},1151:(n,e,i)=>{i.d(e,{Z:()=>c,a:()=>t});var r=i(7294);const a={},s=r.createContext(a);function t(n){const e=r.useContext(s);return r.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:t(n.components),r.createElement(s.Provider,{value:e},n.children)}}}]);