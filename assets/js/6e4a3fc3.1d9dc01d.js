"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[805],{6465:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>l,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var r=i(5893),t=i(1151);const a={sidebar_position:2},s="\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",o={id:"DshanPi-A1/part3/part3-3/03-2-2_FaceDetectionModelDep",title:"\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",description:"\u53c2\u8003\u8d44\u6599\uff1a",source:"@site/docs/DshanPi-A1/part3/part3-3/03-2-2_FaceDetectionModelDep.md",sourceDirName:"DshanPi-A1/part3/part3-3",slug:"/DshanPi-A1/part3/part3-3/03-2-2_FaceDetectionModelDep",permalink:"/docs/DshanPi-A1/part3/part3-3/03-2-2_FaceDetectionModelDep",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/linuxboard-docs/tree/main/docs/DshanPi-A1/part3/part3-3/03-2-2_FaceDetectionModelDep.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"dshanpia1Sidebar",previous:{title:"RKNN\u73af\u5883\u642d\u5efa",permalink:"/docs/DshanPi-A1/part3/part3-3/03-2-1_SetupRKNNEnvironment"},next:{title:"\u4eba\u5f62\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",permalink:"/docs/DshanPi-A1/part3/part3-3/03-2-3_HumanShapeDetectionModelDep"}},d={},c=[{value:"1.\u83b7\u53d6\u539f\u59cb\u6a21\u578b",id:"1\u83b7\u53d6\u539f\u59cb\u6a21\u578b",level:2},{value:"2.\u6a21\u578b\u8f6c\u6362",id:"2\u6a21\u578b\u8f6c\u6362",level:2},{value:"3.\u6a21\u578b\u63a8\u7406",id:"3\u6a21\u578b\u63a8\u7406",level:2},{value:"4.\u89c6\u9891\u6d41\u63a8\u7406",id:"4\u89c6\u9891\u6d41\u63a8\u7406",level:2}];function p(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",children:"\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72"}),"\n",(0,r.jsx)(n.p,{children:"\u53c2\u8003\u8d44\u6599\uff1a"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u4eba\u8138\u6a21\u578b\u4ed3\u5e93\uff1a",(0,r.jsx)(n.a,{href:"https://github.com/biubug6/Pytorch_Retinaface",children:"https://github.com/biubug6/Pytorch_Retinaface"})]}),"\n",(0,r.jsxs)(n.li,{children:["\u4eba\u8138\u68c0\u6d4b\u8d44\u6599\u5305\uff08\u56fd\u5185\u7528\u6237\u63a8\u8350\uff09\uff1a",(0,r.jsx)(n.a,{href:"https://dl.100ask.net/Hardware/MPU/RK3576-DshanPi-A1/utils/Pytorch_Retinaface.zip",children:"https://dl.100ask.net/Hardware/MPU/RK3576-DshanPi-A1/utils/Pytorch_Retinaface.zip"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"1\u83b7\u53d6\u539f\u59cb\u6a21\u578b",children:"1.\u83b7\u53d6\u539f\u59cb\u6a21\u578b"}),"\n",(0,r.jsx)(n.p,{children:"1.\u83b7\u53d6\u6e90\u7801\u4ed3\u5e93"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"git clone https://github.com/biubug6/Pytorch_Retinaface.git\n"})}),"\n",(0,r.jsx)(n.p,{children:"2.\u521b\u5efa\u5e76\u6fc0\u6d3b conda \u73af\u5883"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"conda create -n retinaface python=3.9 -y\nconda activate retinaface\n"})}),"\n",(0,r.jsx)(n.p,{children:"3.\u5b89\u88c5 PyTorch\u4e0e\u5176\u4ed6\u4f9d\u8d56"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"conda install compilers cmake\npip3 install torch torchvision\npip3 install opencv-python pillow matplotlib numpy tensorboard\npip3 install onnx onnxruntime onnxsim\n"})}),"\n",(0,r.jsx)(n.p,{children:"4.\u83b7\u53d6\u9884\u8bad\u7ec3\u6743\u91cd"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"cd Pytorch_Retinaface\n#\u65b0\u5efa\u6743\u91cd\u6587\u4ef6\u5939\nmkdir -p weights\n"})}),"\n",(0,r.jsxs)(n.p,{children:["\u4e0b\u8f7d\u9884\u8bad\u7ec3",(0,r.jsx)(n.a,{href:"https://dl.100ask.net/Hardware/MPU/RK3576-DshanPi-A1/utils/mobilenet0.25_Final.pth",children:"\u6743\u91cd\u6a21\u578b"}),"\u548c",(0,r.jsx)(n.a,{href:"https://dl.100ask.net/Hardware/MPU/RK3576-DshanPi-A1/utils/mobilenetV1X0.25_pretrain.tar",children:"\u9884\u8bad\u7ec3\u6a21\u578b"}),"\uff0c\u4fdd\u5b58\u81f3",(0,r.jsx)(n.code,{children:"weights"}),"\u76ee\u5f55\u4e0b\u3002"]}),"\n",(0,r.jsxs)(n.p,{children:["5.\u4fee\u6539convert_to_onnx.py\u6e90\u7801\u4e2d\u7684",(0,r.jsx)(n.code,{children:"torch.onnx._export"}),"\u4e3a",(0,r.jsx)(n.code,{children:"torch.onnx.export"}),"\uff0c\u4fee\u6539\u5b8c\u6210\u5982\u4e0b\uff1a"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"torch_out = torch.onnx.export(net, inputs, output_onnx, export_params=True, verbose=False,\n                                   input_names=input_names, output_names=output_names)\n"})}),"\n",(0,r.jsx)(n.p,{children:"6.\u6743\u91cd\u6587\u4ef6\u8f6c\u6362ONNX\u6a21\u578b"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"python convert_to_onnx.py\n"})}),"\n",(0,r.jsxs)(n.p,{children:["\u8f6c\u6362\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u770b\u5230",(0,r.jsx)(n.code,{children:"FaceDetector.onnx"}),"\u6a21\u578b\u6587\u4ef6\u3002"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"(retinaface) baiwen@dshanpi-a1:~/Pytorch_Retinaface$ ls\nconvert_to_onnx.py  data       FaceDetector.onnx  LICENSE.MIT  README.md     test_widerface.py  utils    widerface_evaluate\ncurve               detect.py  layers             models       test_fddb.py  train.py           weights\n"})}),"\n",(0,r.jsx)(n.p,{children:"7.\u7b80\u5316\u6a21\u578b"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"python -m onnxsim FaceDetector.onnx retinaface-sim.onnx --input-shape 1,3,640,640\n"})}),"\n",(0,r.jsx)(n.p,{children:"\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'(retinaface) baiwen@dshanpi-a1:~/Pytorch_Retinaface$ python -m onnxsim FaceDetector.onnx retinaface-sim.onnx --input-shape 1,3,640,640\nWARNING: "--input-shape" is renamed to "--overwrite-input-shape". Please use it instead.\nSimplifying...\nFinish! Here is the difference:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503            \u2503 Original Model \u2503 Simplified Model \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Add        \u2502 2              \u2502 2                \u2502\n\u2502 Concat     \u2502 14             \u2502 6                \u2502\n\u2502 Constant   \u2502 147            \u2502 117              \u2502\n\u2502 Conv       \u2502 56             \u2502 56               \u2502\n\u2502 Gather     \u2502 6              \u2502 0                \u2502\n\u2502 LeakyRelu  \u2502 38             \u2502 38               \u2502\n\u2502 Relu       \u2502 3              \u2502 3                \u2502\n\u2502 Reshape    \u2502 9              \u2502 9                \u2502\n\u2502 Resize     \u2502 2              \u2502 2                \u2502\n\u2502 Shape      \u2502 8              \u2502 0                \u2502\n\u2502 Slice      \u2502 2              \u2502 0                \u2502\n\u2502 Softmax    \u2502 1              \u2502 1                \u2502\n\u2502 Transpose  \u2502 9              \u2502 9                \u2502\n\u2502 Unsqueeze  \u2502 6              \u2502 0                \u2502\n\u2502 Model Size \u2502 1.6MiB         \u2502 1.7MiB           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,r.jsxs)(n.p,{children:["\u7b80\u5316\u5b8c\u6210\u540e\u5c06",(0,r.jsx)(n.code,{children:"retinaface-sim.onnx"}),"\u62f7\u8d1d\u81f3RetinaFace\u5de5\u4f5c\u76ee\u5f55\u7684\u6a21\u578b\u8def\u5f84\u4e0b\uff1a"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"cp retinaface-sim.onnx ~/Projects/rknn_model_zoo/examples/RetinaFace/model\n"})}),"\n",(0,r.jsx)(n.h2,{id:"2\u6a21\u578b\u8f6c\u6362",children:"2.\u6a21\u578b\u8f6c\u6362"}),"\n",(0,r.jsxs)(n.p,{children:["1.\u4f7f\u7528Conda\u6fc0\u6d3b",(0,r.jsx)(n.code,{children:"rknn-toolkit2"}),"\u73af\u5883"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"conda activate rknn-toolkit2\n"})}),"\n",(0,r.jsx)(n.p,{children:"2.\u8fdb\u5165RetinaFace\u6a21\u578b\u8f6c\u6362\u76ee\u5f55"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"cd ~/Projects/rknn_model_zoo/examples/RetinaFace/python\n"})}),"\n",(0,r.jsx)(n.p,{children:"3.\u6267\u884c\u6a21\u578b\u8f6c\u6362"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"python3 convert.py ../model/retinaface-sim.onnx rk3576\n"})}),"\n",(0,r.jsx)(n.p,{children:"\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"(rknn-toolkit2) baiwen@dshanpi-a1:~/Projects/rknn_model_zoo/examples/RetinaFace/python$ python3 convert.py ../model/retinaface-sim.onnx rk3576\nI rknn-toolkit2 version: 2.3.2\n--\x3e Config model\ndone\n--\x3e Loading model\nI Loading : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 117/117 [00:00<00:00, 11380.65it/s]\ndone\n--\x3e Building model\nI OpFusing 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00<00:00, 297.57it/s]\nI OpFusing 1 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00<00:00, 104.34it/s]\nI OpFusing 0 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:02<00:00, 46.89it/s]\nI OpFusing 1 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:02<00:00, 44.28it/s]\nI OpFusing 2 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:03<00:00, 32.20it/s]\nW build: found outlier value, this may affect quantization accuracy\n                        const name          abs_mean    abs_std     outlier value\n                        onnx::Conv_634      0.69        0.83        21.961\nI GraphPreparing : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 123/123 [00:00<00:00, 882.71it/s]\nI Quantizating : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 123/123 [00:00<00:00, 152.58it/s]\nI OpFusing 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00<00:00, 448.58it/s]\nI OpFusing 1 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00<00:00, 433.87it/s]\nI OpFusing 2 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00<00:00, 424.22it/s]\nW build: The default input dtype of 'input0' is changed from 'float32' to 'int8' in rknn model for performance!\n                       Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of 'output0' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '592' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nI rknn building ...\nI rknn building done.\ndone\n--\x3e Export rknn model\ndone\n(rknn-toolkit2) baiwen@dshanpi-a1:~/Projects/rknn_model_zoo/examples/RetinaFace/python$ ls ../model/\ndataset.txt  download_model.sh  RetinaFace.rknn  retinaface-sim.onnx  test.jpg\n"})}),"\n",(0,r.jsxs)(n.p,{children:["\u8fd0\u884c\u5b8c\u6210\u540e\u53ef\u4ee5\u5728",(0,r.jsx)(n.code,{children:"../model/"}),"\u76ee\u5f55\u4e0b\u53ef\u4ee5\u770b\u5230\u8f6c\u6362\u5b8c\u6210\u540e\u7684\u7aef\u4fa7\u63a8\u7406\u6a21\u578b",(0,r.jsx)(n.code,{children:"RetinaFace.rknn"}),"\u3002"]}),"\n",(0,r.jsx)(n.h2,{id:"3\u6a21\u578b\u63a8\u7406",children:"3.\u6a21\u578b\u63a8\u7406"}),"\n",(0,r.jsx)(n.p,{children:"1.\u4fee\u6539\u4ee3\u7801\u652f\u6301640*640\u7684\u6a21\u578b\u8f93\u5165\uff0c"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"vi RetinaFace.py\n"})}),"\n",(0,r.jsx)(n.p,{children:"\u627e\u5230166\u884c\u5de6\u53f3\uff0c\u5c06\u539f\u6765\u7684\u6a21\u578b\u5bbd\u9ad8\u4ee3\u7801\u4fee\u6539\u4e3a640\uff0c\u4e0b\u9762\u4e3a\u539f\u59cb\u4ee3\u7801\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:" model_height, model_width = (320, 320)\n"})}),"\n",(0,r.jsx)(n.p,{children:"\u4fee\u6539\u4e3a\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:" model_height, model_width = (640, 640)\n"})}),"\n",(0,r.jsx)(n.p,{children:"2.\u6267\u884c\u63a8\u7406\u6d4b\u8bd5\u4ee3\u7801\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"python3 RetinaFace.py --model_path ../model/RetinaFace.rknn --target rk3576\n"})}),"\n",(0,r.jsx)(n.p,{children:"\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"(rknn-toolkit2) baiwen@dshanpi-a1:~/Projects/rknn_model_zoo/examples/RetinaFace/python$ python3 RetinaFace.py --model_path ../model/RetinaFace.rknn --target rk3576\nI rknn-toolkit2 version: 2.3.2\ndone\n--\x3e Init runtime environment\nI target set by user is: rk3576\ndone\n--\x3e Running model\nW inference: The 'data_format' is not set, and its default value is 'nhwc'!\nimage_size: (640, 640)  num_priors= 16800\nface @ (309 78 485 304) 0.999023\nsave image in ./result.jpg\n"})}),"\n",(0,r.jsxs)(n.p,{children:["\u53ef\u4ee5\u770b\u5230\u8fd0\u884c\u5b8c\u6210\u540e\u4f1a\u4fdd\u5b58\u7ed3\u679c\u56fe\u50cf",(0,r.jsx)(n.code,{children:"result.jpg"}),"\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u3002"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"image-20250819100935547",src:i(294).Z+"",width:"640",height:"427"})}),"\n",(0,r.jsx)(n.h2,{id:"4\u89c6\u9891\u6d41\u63a8\u7406",children:"4.\u89c6\u9891\u6d41\u63a8\u7406"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"\u5f00\u59cb\u524d\u8bf7\u6ce8\u610f\uff0c\u8bf7\u52a1\u5fc5\u63a5\u5165USB\u6444\u50cf\u5934\uff0c\u5e76\u786e\u8ba4/dev/\u76ee\u5f55\u4e0b\u5b58\u5728video0\u8bbe\u5907\u8282\u70b9\uff01\uff01\uff01"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["1.\u65b0\u5efa\u7a0b\u5e8f\u6587\u4ef6",(0,r.jsx)(n.code,{children:"RetinaFace_video.py"}),",\u586b\u5165\u4e00\u4e0b\u5185\u5bb9\uff1a"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"import os\nimport sys\nimport urllib\nimport urllib.request\nimport time\nimport numpy as np\nimport argparse\nimport cv2\nfrom math import ceil\nfrom itertools import product as product\n\nfrom rknn.api import RKNN\n\ndef letterbox_resize(image, size, bg_color):\n    \"\"\"\n    letterbox_resize the image according to the specified size\n    :param image: input image, which can be a NumPy array or file path\n    :param size: target size (width, height)\n    :param bg_color: background filling data \n    :return: processed image\n    \"\"\"\n    if isinstance(image, str):\n        image = cv2.imread(image)\n\n    target_width, target_height = size\n    image_height, image_width, _ = image.shape\n\n    # Calculate the adjusted image size\n    aspect_ratio = min(target_width / image_width, target_height / image_height)\n    new_width = int(image_width * aspect_ratio)\n    new_height = int(image_height * aspect_ratio)\n\n    # Use cv2.resize() for proportional scaling\n    image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n\n    # Create a new canvas and fill it\n    result_image = np.ones((target_height, target_width, 3), dtype=np.uint8) * bg_color\n    offset_x = (target_width - new_width) // 2\n    offset_y = (target_height - new_height) // 2\n    result_image[offset_y:offset_y + new_height, offset_x:offset_x + new_width] = image\n    return result_image, aspect_ratio, offset_x, offset_y\n\ndef PriorBox(image_size): #image_size Support (320,320) and (640,640)\n    anchors = []\n    min_sizes = [[16, 32], [64, 128], [256, 512]]\n    steps = [8, 16, 32]\n    feature_maps = [[ceil(image_size[0] / step), ceil(image_size[1] / step)] for step in steps]\n    for k, f in enumerate(feature_maps):\n        min_sizes_ = min_sizes[k]\n        for i, j in product(range(f[0]), range(f[1])):\n            for min_size in min_sizes_:\n                s_kx = min_size / image_size[1]\n                s_ky = min_size / image_size[0]\n                dense_cx = [x * steps[k] / image_size[1] for x in [j + 0.5]]\n                dense_cy = [y * steps[k] / image_size[0] for y in [i + 0.5]]\n                for cy, cx in product(dense_cy, dense_cx):\n                    anchors += [cx, cy, s_kx, s_ky]\n    output = np.array(anchors).reshape(-1, 4)\n    print(\"image_size:\",image_size,\" num_priors=\",output.shape[0])\n    return output\n\ndef box_decode(loc, priors):\n    \"\"\"Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    \"\"\"\n    variances = [0.1, 0.2]\n    boxes = np.concatenate((\n        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n        priors[:, 2:] * np.exp(loc[:, 2:] * variances[1])), axis=1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\ndef decode_landm(pre, priors):\n    \"\"\"Decode landm from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        pre (tensor): landm predictions for loc layers,\n            Shape: [num_priors,10]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded landm predictions\n    \"\"\"\n    variances = [0.1, 0.2]\n    landmarks = np.concatenate((\n        priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n        priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n        priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n        priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n        priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:]\n    ), axis=1)\n    return landmarks\n\ndef nms(dets, thresh):\n    \"\"\"Pure Python NMS baseline.\"\"\"\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='RetinaFace Real-time Camera Demo', add_help=True)\n    parser.add_argument('--model_path', type=str, required=True,\n                        help='model path, could be .rknn file')\n    parser.add_argument('--target', type=str,\n                        default='rk3576', help='target RKNPU platform')\n    parser.add_argument('--device_id', type=str,\n                        default=None, help='device id')\n    args = parser.parse_args()\n\n    # 1. \u52a0\u8f7d RKNN \u6a21\u578b\n    rknn = RKNN(verbose=False)\n    rknn.load_rknn(args.model_path)\n    rknn.init_runtime(target=args.target)\n    print('Model & runtime ready.')\n\n    # 2. \u6253\u5f00\u6444\u50cf\u5934\n    cap = cv2.VideoCapture(0)          # /dev/video0\n    if not cap.isOpened():\n        print('Cannot open camera.')\n        exit(-1)\n\n    model_height, model_width = 640, 640\n    priors = PriorBox(image_size=(model_height, model_width))\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        img_height, img_width = frame.shape[:2]\n\n        # 3. \u9884\u5904\u7406\uff08letterbox + RGB\uff09\n        letterbox_img, aspect_ratio, offset_x, offset_y = letterbox_resize(\n            frame, (model_height, model_width), 114)\n        infer_img = letterbox_img[..., ::-1]          # BGR\u2192RGB\n        infer_img = infer_img.astype(np.float32)\n\n        # 4. RKNN \u63a8\u7406\n        loc, conf, landmarks = rknn.inference(inputs=[infer_img])\n\n        # 5. \u540e\u5904\u7406\n        boxes = box_decode(loc.squeeze(0), priors)\n        scale = np.array([model_width, model_height,\n                          model_width, model_height])\n        boxes = boxes * scale\n        boxes[..., 0::2] = np.clip((boxes[..., 0::2] - offset_x) / aspect_ratio, 0, img_width)\n        boxes[..., 1::2] = np.clip((boxes[..., 1::2] - offset_y) / aspect_ratio, 0, img_height)\n\n        scores = conf.squeeze(0)[:, 1]\n        landmarks = decode_landm(landmarks.squeeze(0), priors)\n        scale_lm = np.array([model_width, model_height] * 5)\n        landmarks = landmarks * scale_lm\n        landmarks[..., 0::2] = np.clip((landmarks[..., 0::2] - offset_x) / aspect_ratio, 0, img_width)\n        landmarks[..., 1::2] = np.clip((landmarks[..., 1::2] - offset_y) / aspect_ratio, 0, img_height)\n\n        inds = np.where(scores > 0.4)[0]         # \u9608\u503c\u53ef\u8c03\n        dets = np.hstack((boxes[inds], scores[inds, np.newaxis])).astype(np.float32)\n        keep = nms(dets, 0.5)\n        dets = dets[keep]\n        landmarks = landmarks[inds][keep]\n\n        # 6. \u753b\u6846\u753b\u70b9\n        for det in dets:\n            x1, y1, x2, y2 = det[:4].astype(int)\n            score = det[4] \n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n            cv2.putText(frame, f'{score:.2f}', (x1, y1 - 5),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n        for lm in landmarks:\n            for k in range(5):\n                cv2.circle(frame, (int(lm[2*k]), int(lm[2*k+1])), 2, (0, 255, 255), -1)\n\n        # 7. \u5b9e\u65f6\u663e\u793a\n        cv2.imshow('RetinaFace Real-time', frame)\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n\n    # 8. \u6e05\u7406\n    cap.release()\n    cv2.destroyAllWindows()\n    rknn.release()\n"})}),"\n",(0,r.jsx)(n.p,{children:"2.\u8fd0\u884c\u89c6\u9891\u6d41\u63a8\u7406"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"python3 RetinaFace_video.py --model_path ../model/RetinaFace.rknn --target rk3576\n"})})]})}function l(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},294:(e,n,i)=>{i.d(n,{Z:()=>r});const r=i.p+"assets/images/image-20250819100935547-fd601f5f7a07e462192475570438c1b1.png"},1151:(e,n,i)=>{i.d(n,{Z:()=>o,a:()=>s});var r=i(7294);const t={},a=r.createContext(t);function s(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);