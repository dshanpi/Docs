"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[67],{915:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var t=o(5893),s=o(1151);const r={sidebar_position:5},a="\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u90e8\u7f72",i={id:"DshanPi-A1/part3/part3-3/03-2-5_InstanceSegmentationModelDep",title:"\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u90e8\u7f72",description:"\u53c2\u8003\u8d44\u6599\uff1a",source:"@site/docs/DshanPi-A1/part3/part3-3/03-2-5_InstanceSegmentationModelDep.md",sourceDirName:"DshanPi-A1/part3/part3-3",slug:"/DshanPi-A1/part3/part3-3/03-2-5_InstanceSegmentationModelDep",permalink:"/en/docs/DshanPi-A1/part3/part3-3/03-2-5_InstanceSegmentationModelDep",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/linuxboard-docs/tree/main/docs/DshanPi-A1/part3/part3-3/03-2-5_InstanceSegmentationModelDep.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5},sidebar:"dshanpia1Sidebar",previous:{title:"\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",permalink:"/en/docs/DshanPi-A1/part3/part3-3/03-2-4_TargetDetectionModelDep"},next:{title:"\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",permalink:"/en/docs/DshanPi-A1/part3/part3-3/03-2-6_RotationalObjectDetectionModelDep"}},l={},c=[{value:"1.\u83b7\u53d6\u539f\u59cb\u6a21\u578b",id:"1\u83b7\u53d6\u539f\u59cb\u6a21\u578b",level:2},{value:"2.\u6a21\u578b\u8f6c\u6362",id:"2\u6a21\u578b\u8f6c\u6362",level:2},{value:"3.\u6a21\u578b\u63a8\u7406",id:"3\u6a21\u578b\u63a8\u7406",level:2},{value:"4.\u89c6\u9891\u6d41\u63a8\u7406",id:"4\u89c6\u9891\u6d41\u63a8\u7406",level:2}];function p(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u90e8\u7f72",children:"\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u90e8\u7f72"}),"\n",(0,t.jsx)(n.p,{children:"\u53c2\u8003\u8d44\u6599\uff1a"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u6a21\u578b\u4ed3\u5e93\uff1a",(0,t.jsx)(n.a,{href:"https://github.com/airockchip/ultralytics_yolov8",children:"https://github.com/airockchip/ultralytics_yolov8"})]}),"\n",(0,t.jsxs)(n.li,{children:["\u6a21\u578b\u6587\u6863\uff1a",(0,t.jsx)(n.a,{href:"https://github.com/airockchip/ultralytics_yolov8/blob/main/RKOPT_README.zh-CN.md",children:"ultralytics_yolov8/RKOPT_README.zh-CN.md at main \xb7 airockchip/ultralytics_yolov8"})]}),"\n",(0,t.jsxs)(n.li,{children:["\u6a21\u578b\u6e90\u7801\u8d44\u6599\u5305\uff08\u56fd\u5185\u7528\u6237\u63a8\u8350\uff09\uff1a",(0,t.jsx)(n.a,{href:"https://dl.100ask.net/Hardware/MPU/RK3576-DshanPi-A1/utils/ultralytics_yolov8.zip",children:"https://dl.100ask.net/Hardware/MPU/RK3576-DshanPi-A1/utils/ultralytics_yolov8.zip"})]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"1\u83b7\u53d6\u539f\u59cb\u6a21\u578b",children:"1.\u83b7\u53d6\u539f\u59cb\u6a21\u578b"}),"\n",(0,t.jsx)(n.p,{children:"1.\u8fdb\u5165\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4ed3\u5e93\uff1a"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"git clone https://github.com/airockchip/ultralytics_yolov8\ncd ultralytics_yolov8\n"})}),"\n",(0,t.jsx)(n.p,{children:"2.\u4f7f\u7528conda\u521b\u5efa\u73af\u5883"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"conda create -name yolov8 python=3.9\nconda activate yolov8\n"})}),"\n",(0,t.jsx)(n.p,{children:"3.\u5b89\u88c5yolov8\u76f8\u5173\u4f9d\u8d56"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"pip3 install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 ultralytics==8.3.31 onnx==1.17.0 onnxruntime==1.8.0 onnxsim==0.4.36\n"})}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"\u5982\u5df2\u5b89\u88c5\u8fc7\u53ef\u5ffd\u7565\uff01"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["4.\u4fee\u6539",(0,t.jsx)(n.code,{children:"./ultralytics/cfg/default.yaml"}),"\u914d\u7f6e\u6587\u4ef6"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"vi ./ultralytics/cfg/default.yaml\n"})}),"\n",(0,t.jsxs)(n.p,{children:["\u5c06\u539f\u59cb\u7684",(0,t.jsx)(n.code,{children:"model: yolov8n.pt"}),"\u4e3a\uff1a"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"model: yolov8n-seg.pt\n"})}),"\n",(0,t.jsx)(n.p,{children:"5.\u5bfc\u51faONNX\u6a21\u578b"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"python ultralytics/engine/exporter.py\n"})}),"\n",(0,t.jsxs)(n.p,{children:["\u5982\u679c\u65e0\u6cd5\u4e0b\u8f7d\u6a21\u578b\u53ef\u76f4\u63a5\u8bbf\u95ee",(0,t.jsx)(n.a,{href:"https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n-seg.pt",children:"yolov8n-seg.pt"}),"\uff0c\u4e0b\u8f7d\u540e\u653e\u5728",(0,t.jsx)(n.code,{children:"ultralytics_yolov8"}),"\u76ee\u5f55\u4e0b\uff0c\u518d\u6b21\u6267\u884c\u3002"]}),"\n",(0,t.jsx)(n.p,{children:"\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"(yolov8) baiwen@dshanpi-a1:~/ultralytics_yolov8$ python ultralytics/engine/exporter.py\nUltralytics YOLOv8.2.82 \ud83d\ude80 Python-3.9.23 torch-2.4.1 CPU (Cortex-A53)\nYOLOv8n-seg summary (fused): 195 layers, 3,404,320 parameters, 0 gradients, 12.6 GFLOPs\n\nPyTorch: starting from 'yolov8n-seg.pt' with input shape (16, 3, 640, 640) BCHW and output shape(s) ((16, 64, 80, 80), (16, 80, 80, 80), (16, 1, 80, 80), (16, 32, 80, 80), (16, 64, 40, 40), (16, 80, 40, 40), (16, 1, 40, 40), (16, 32, 40, 40), (16, 64, 20, 20), (16, 80, 20, 20), (16, 1, 20, 20), (16, 32, 20, 20), (16, 32, 160, 160)) (6.7 MB)\n\nRKNN: starting export with torch 2.4.1...\n\nRKNN: feed yolov8n-seg.onnx to RKNN-Toolkit or RKNN-Toolkit2 to generate RKNN model.\nRefer https://github.com/airockchip/rknn_model_zoo/tree/main/models/CV/object_detection/yolo\nRKNN: export success \u2705 3.2s, saved as 'yolov8n-seg.onnx' (13.0 MB)\n\nExport complete (34.3s)\nResults saved to /home/baiwen/ultralytics_yolov8\nPredict:         yolo predict task=segment model=yolov8n-seg.onnx imgsz=640\nValidate:        yolo val task=segment model=yolov8n-seg.onnx imgsz=640 data=coco.yaml\nVisualize:       https://netron.app\n"})}),"\n",(0,t.jsxs)(n.p,{children:["\u6267\u884c\u5b8c\u6210\u540e\u53ef\u4ee5\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u770b\u5230ONNX\u6a21\u578b\u6587\u4ef6",(0,t.jsx)(n.code,{children:"yolov8n-seg.onnx"}),"\u3002"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"(yolov8) baiwen@dshanpi-a1:~/ultralytics_yolov8$ ls\nCITATION.cff     docs      mkdocs.yml      README.zh-CN.md        tests                 yolov8n.onnx      yolov8n-seg.pt\nCONTRIBUTING.md  examples  pyproject.toml  RKOPT_README.md        ultralytics           yolov8n.pt\ndocker           LICENSE   README.md       RKOPT_README.zh-CN.md  ultralytics.egg-info  yolov8n-seg.onnx\n"})}),"\n",(0,t.jsx)(n.p,{children:"\u5c06\u5bfc\u51fa\u7684ONNX\u6a21\u578b\u62f7\u8d1d\u81f3yolov8\u6a21\u578b\u76ee\u5f55\u3002"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"cp yolov8n-seg.onnx ~/Projects/rknn_model_zoo/examples/yolov8_seg/model\n"})}),"\n",(0,t.jsx)(n.h2,{id:"2\u6a21\u578b\u8f6c\u6362",children:"2.\u6a21\u578b\u8f6c\u6362"}),"\n",(0,t.jsxs)(n.p,{children:["1.\u4f7f\u7528Conda\u6fc0\u6d3b",(0,t.jsx)(n.code,{children:"rknn-toolkit2"}),"\u73af\u5883"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"conda activate rknn-toolkit2\n"})}),"\n",(0,t.jsx)(n.p,{children:"2.\u8fdb\u5165yolov8\u6a21\u578b\u8f6c\u6362\u76ee\u5f55"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"cd ~/Projects/rknn_model_zoo/examples/yolov8_seg/python\n"})}),"\n",(0,t.jsx)(n.p,{children:"3.\u6267\u884c\u6a21\u578b\u8f6c\u6362"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"python3 convert.py ../model/yolov8n-seg.onnx rk3576\n"})}),"\n",(0,t.jsx)(n.p,{children:"\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"(rknn-toolkit2) baiwen@dshanpi-a1:~/Projects/rknn_model_zoo/examples/yolov8_seg/python$ python3 convert.py ../model/yolov8n-seg.onnx rk3576\nI rknn-toolkit2 version: 2.3.2\n--\x3e Config model\ndone\n--\x3e Loading model\nI Loading : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 152/152 [00:00<00:00, 8900.13it/s]\ndone\n--\x3e Building model\nI OpFusing 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00<00:00, 150.96it/s]\nI OpFusing 1 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01<00:00, 86.53it/s]\nI OpFusing 0 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01<00:00, 73.52it/s]\nI OpFusing 1 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01<00:00, 65.14it/s]\nI OpFusing 2 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04<00:00, 22.36it/s]\nW build: found outlier value, this may affect quantization accuracy\n                        const name                        abs_mean    abs_std     outlier value\n                        model.22.cv3.1.1.conv.weight      0.12        0.18        -12.310\nI GraphPreparing : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 183/183 [00:00<00:00, 1047.63it/s]\nI Quantizating : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 183/183 [00:41<00:00,  4.45it/s]\nW build: The default input dtype of 'images' is changed from 'float32' to 'int8' in rknn model for performance!\n                       Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '375' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of 'onnx::ReduceSum_383' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '388' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '354' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '395' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of 'onnx::ReduceSum_403' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '407' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '361' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '414' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of 'onnx::ReduceSum_422' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '426' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '368' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '347' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nI rknn building ...\nI rknn building done.\ndone\n--\x3e Export rknn model\ndone\n"})}),"\n",(0,t.jsx)(n.p,{children:"\u53ef\u4ee5\u770b\u5230\u8f6c\u6362\u5b8c\u6210\u540e\u5728model\u76ee\u5f55\u4e0b\u770b\u5230\u7aef\u4fa7\u7684RKNN\u6a21\u578b\u3002"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"(rknn-toolkit2) baiwen@dshanpi-a1:~/Projects/rknn_model_zoo/examples/yolov8_seg/python$ ls ../model\nbus.jpg  coco_80_labels_list.txt  download_model.sh  yolov8n-seg.onnx  yolov8_seg.rknn\n"})}),"\n",(0,t.jsx)(n.h2,{id:"3\u6a21\u578b\u63a8\u7406",children:"3.\u6a21\u578b\u63a8\u7406"}),"\n",(0,t.jsx)(n.p,{children:"\u7531\u4e8e\u4ee3\u7801\u9700\u8981\u5b89\u88c5pytorch\uff0c\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"pip3 install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0\npip install pycocotools\n"})}),"\n",(0,t.jsx)(n.p,{children:"\u6267\u884c\u63a8\u7406\u6d4b\u8bd5\u4ee3\u7801\uff1a"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"python3 yolov8_seg.py --model_path ../model/yolov8_seg.rknn --target rk3576 --img_save\n"})}),"\n",(0,t.jsx)(n.p,{children:"\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"(rknn-toolkit2) baiwen@dshanpi-a1:~/Projects/rknn_model_zoo/examples/yolov8_seg/python$ python3 yolov8_seg.py --model_path ../model/yolov8_seg.rknn --target rk3576 --img_save\nI rknn-toolkit2 version: 2.3.2\n--\x3e Init runtime environment\nI target set by user is: rk3576\ndone\nModel-../model/yolov8_seg.rknn is rknn model, starting val\nW inference: The 'data_format' is not set, and its default value is 'nhwc'!\n\n\nIMG: bus.jpg\nperson @ (209 241 285 509) 0.860\nbus  @ (94 137 557 439) 0.829\nperson @ (109 235 223 535) 0.829\nperson @ (475 233 560 521) 0.790\nThe segmentation results have been saved to ./result/bus.jpg\n"})}),"\n",(0,t.jsxs)(n.p,{children:["\u8fd0\u884c\u5b8c\u6210\u540e\u53ef\u4ee5\u5728",(0,t.jsx)(n.code,{children:"./result/bus.jpg"}),"\u8def\u5f84\u770b\u5230\u7ed3\u679c\u56fe\u50cf\u3002"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"image-20250819161239333",src:o(748).Z+"",width:"640",height:"640"})}),"\n",(0,t.jsx)(n.h2,{id:"4\u89c6\u9891\u6d41\u63a8\u7406",children:"4.\u89c6\u9891\u6d41\u63a8\u7406"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"\u5f00\u59cb\u524d\u8bf7\u6ce8\u610f\uff0c\u8bf7\u52a1\u5fc5\u63a5\u5165USB\u6444\u50cf\u5934\uff0c\u5e76\u786e\u8ba4/dev/\u76ee\u5f55\u4e0b\u5b58\u5728video0\u8bbe\u5907\u8282\u70b9\uff01\uff01\uff01"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["1.\u65b0\u5efa\u7a0b\u5e8f\u6587\u4ef6",(0,t.jsx)(n.code,{children:"yolov8_seg_video.py.py"}),",\u586b\u5165\u4e00\u4e0b\u5185\u5bb9\uff1a"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'import os\nimport cv2\nimport sys\nimport argparse\nimport torch\nimport numpy as np\nimport torch\nimport torchvision\nimport torch.nn.functional as F\nfrom pathlib import Path\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\n# add path\nrealpath = os.path.abspath(__file__)\n_sep = os.path.sep\nrealpath = realpath.split(_sep)\nsys.path.append(os.path.join(realpath[0]+_sep, *realpath[1:realpath.index(\'rknn_model_zoo\')+1]))\n\nfrom py_utils.coco_utils import COCO_test_helper\n\n\nOBJ_THRESH = 0.25\nNMS_THRESH = 0.45\nMAX_DETECT = 300\n\n# The follew two param is for mAP test\n# OBJ_THRESH = 0.001\n# NMS_THRESH = 0.65\n\nIMG_SIZE = (640, 640)  # (width, height), such as (1280, 736)\n\nCLASSES = ("person", "bicycle", "car","motorbike ","aeroplane ","bus ","train","truck ","boat","traffic light",\n           "fire hydrant","stop sign ","parking meter","bench","bird","cat","dog ","horse ","sheep","cow","elephant",\n           "bear","zebra ","giraffe","backpack","umbrella","handbag","tie","suitcase","frisbee","skis","snowboard","sports ball","kite",\n           "baseball bat","baseball glove","skateboard","surfboard","tennis racket","bottle","wine glass","cup","fork","knife ",\n           "spoon","bowl","banana","apple","sandwich","orange","broccoli","carrot","hot dog","pizza ","donut","cake","chair","sofa",\n           "pottedplant","bed","diningtable","toilet ","tvmonitor","laptop\t","mouse\t","remote ","keyboard ","cell phone","microwave ",\n           "oven ","toaster","sink","refrigerator ","book","clock","vase","scissors ","teddy bear ","hair drier", "toothbrush ")\n\ncoco_id_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,\n         35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n         64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n\nclass Colors:\n    # Ultralytics color palette https://ultralytics.com/\n    def __init__(self):\n        # hex = matplotlib.colors.TABLEAU_COLORS.values()\n        hexs = (\'FF3838\', \'FF9D97\', \'FF701F\', \'FFB21D\', \'CFD231\', \'48F90A\', \'92CC17\', \'3DDB86\', \'1A9334\', \'00D4BB\',\n                \'2C99A8\', \'00C2FF\', \'344593\', \'6473FF\', \'0018EC\', \'8438FF\', \'520085\', \'CB38FF\', \'FF95C8\', \'FF37C7\')\n        self.palette = [self.hex2rgb(f\'#{c}\') for c in hexs]\n        self.n = len(self.palette)\n\n    def __call__(self, i, bgr=False):\n        c = self.palette[int(i) % self.n]\n        return (c[2], c[1], c[0]) if bgr else c\n\n    @staticmethod\n    def hex2rgb(h):  # rgb order (PIL)\n        return tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef filter_boxes(boxes, box_confidences, box_class_probs, seg_part):\n    """Filter boxes with object threshold.\n    """\n    box_confidences = box_confidences.reshape(-1)\n    candidate, class_num = box_class_probs.shape\n\n    class_max_score = np.max(box_class_probs, axis=-1)\n    classes = np.argmax(box_class_probs, axis=-1)\n\n    _class_pos = np.where(class_max_score * box_confidences >= OBJ_THRESH)\n    scores = (class_max_score * box_confidences)[_class_pos]\n\n    boxes = boxes[_class_pos]\n    classes = classes[_class_pos]\n    seg_part = (seg_part * box_confidences.reshape(-1, 1))[_class_pos]\n\n    return boxes, classes, scores, seg_part\n\ndef dfl(position):\n    # Distribution Focal Loss (DFL)\n    x = torch.tensor(position)\n    n,c,h,w = x.shape\n    p_num = 4\n    mc = c//p_num\n    y = x.reshape(n,p_num,mc,h,w)\n    y = y.softmax(2)\n    acc_metrix = torch.tensor(range(mc)).float().reshape(1,1,mc,1,1)\n    y = (y*acc_metrix).sum(2)\n    return y.numpy()\n\ndef box_process(position):\n    grid_h, grid_w = position.shape[2:4]\n    col, row = np.meshgrid(np.arange(0, grid_w), np.arange(0, grid_h))\n    col = col.reshape(1, 1, grid_h, grid_w)\n    row = row.reshape(1, 1, grid_h, grid_w)\n    grid = np.concatenate((col, row), axis=1)\n    stride = np.array([IMG_SIZE[1]//grid_h, IMG_SIZE[0] //grid_w]).reshape(1, 2, 1, 1)\n\n    position = dfl(position)\n    box_xy  = grid +0.5 -position[:,0:2,:,:]\n    box_xy2 = grid +0.5 +position[:,2:4,:,:]\n    xyxy = np.concatenate((box_xy*stride, box_xy2*stride), axis=1)\n\n    return xyxy\n\ndef post_process(input_data):\n    # input_data[0], input_data[4], and input_data[8] are detection box information\n    # input_data[1], input_data[5], and input_data[9] are category score information\n    # input_data[2], input_data[6], and input_data[10] are confidence score information\n    # input_data[3], input_data[7], and input_data[11] are segmentation information\n    # input_data[12] is the proto information\n    proto = input_data[-1]\n    boxes, scores, classes_conf, seg_part = [], [], [], []\n    defualt_branch=3\n    pair_per_branch = len(input_data)//defualt_branch\n\n    for i in range(defualt_branch):\n        boxes.append(box_process(input_data[pair_per_branch*i]))\n        classes_conf.append(input_data[pair_per_branch*i+1])\n        scores.append(np.ones_like(input_data[pair_per_branch*i+1][:,:1,:,:], dtype=np.float32))\n        seg_part.append(input_data[pair_per_branch*i+3])\n\n    def sp_flatten(_in):\n        ch = _in.shape[1]\n        _in = _in.transpose(0,2,3,1)\n        return _in.reshape(-1, ch)\n\n    boxes = [sp_flatten(_v) for _v in boxes]\n    classes_conf = [sp_flatten(_v) for _v in classes_conf]\n    scores = [sp_flatten(_v) for _v in scores]\n    seg_part = [sp_flatten(_v) for _v in seg_part]\n\n    boxes = np.concatenate(boxes)\n    classes_conf = np.concatenate(classes_conf)\n    scores = np.concatenate(scores)\n    seg_part = np.concatenate(seg_part)\n\n    # filter according to threshold\n    boxes, classes, scores, seg_part = filter_boxes(boxes, scores, classes_conf, seg_part)\n\n    zipped = zip(boxes, classes, scores, seg_part)\n    sort_zipped = sorted(zipped, key=lambda x: (x[2]), reverse=True)\n    result = zip(*sort_zipped)\n\n    max_nms = 30000\n    n = boxes.shape[0]  # number of boxes\n    if not n:\n        return None, None, None, None\n    elif n > max_nms:  # excess boxes\n        boxes, classes, scores, seg_part = [np.array(x[:max_nms]) for x in result]\n    else:\n        boxes, classes, scores, seg_part = [np.array(x) for x in result]\n\n    # nms\n    nboxes, nclasses, nscores, nseg_part = [], [], [], []\n    agnostic = 0\n    max_wh = 7680\n    c = classes * (0 if agnostic else max_wh)\n    ids = torchvision.ops.nms(torch.tensor(boxes, dtype=torch.float32) + torch.tensor(c, dtype=torch.float32).unsqueeze(-1),\n                              torch.tensor(scores, dtype=torch.float32), NMS_THRESH)\n    real_keeps = ids.tolist()[:MAX_DETECT]\n    nboxes.append(boxes[real_keeps])\n    nclasses.append(classes[real_keeps])\n    nscores.append(scores[real_keeps])\n    nseg_part.append(seg_part[real_keeps])\n\n    if not nclasses and not nscores:\n        return None, None, None, None\n\n    boxes = np.concatenate(nboxes)\n    classes = np.concatenate(nclasses)\n    scores = np.concatenate(nscores)\n    seg_part = np.concatenate(nseg_part)\n\n    ph, pw = proto.shape[-2:]\n    proto = proto.reshape(seg_part.shape[-1], -1)\n    seg_img = np.matmul(seg_part, proto)\n    seg_img = sigmoid(seg_img)\n    seg_img = seg_img.reshape(-1, ph, pw)\n\n    seg_threadhold = 0.5\n\n    # crop seg outside box\n    seg_img = F.interpolate(torch.tensor(seg_img)[None], torch.Size([640, 640]), mode=\'bilinear\', align_corners=False)[0]\n    seg_img_t = _crop_mask(seg_img,torch.tensor(boxes) )\n\n    seg_img = seg_img_t.numpy()\n    seg_img = seg_img > seg_threadhold\n    return boxes, classes, scores, seg_img\n\ndef draw(image, boxes, scores, classes):\n    for box, score, cl in zip(boxes, scores, classes):\n        top, left, right, bottom = [int(_b) for _b in box]\n        print("%s @ (%d %d %d %d) %.3f" % (CLASSES[cl], top, left, right, bottom, score))\n        cv2.rectangle(image, (top, left), (right, bottom), (255, 0, 0), 2)\n        cv2.putText(image, \'{0} {1:.2f}\'.format(CLASSES[cl], score),\n                    (top, left - 6), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n\ndef _crop_mask(masks, boxes):\n    """\n    "Crop" predicted masks by zeroing out everything not in the predicted bbox.\n    Vectorized by Chong (thanks Chong).\n\n    Args:\n        - masks should be a size [h, w, n] tensor of masks\n        - boxes should be a size [n, 4] tensor of bbox coords in relative point form\n    """\n\n    n, h, w = masks.shape\n    x1, y1, x2, y2 = torch.chunk(boxes[:, :, None], 4, 1)  # x1 shape(1,1,n)\n    r = torch.arange(w, device=masks.device, dtype=x1.dtype)[None, None, :]  # rows shape(1,w,1)\n    c = torch.arange(h, device=masks.device, dtype=x1.dtype)[None, :, None]  # cols shape(h,1,1)\n    \n    return masks * ((r >= x1) * (r < x2) * (c >= y1) * (c < y2))\n\ndef merge_seg(image, seg_img, classes):\n    color = Colors()\n    for i in range(len(seg_img)):\n        seg = seg_img[i]\n        seg = seg.astype(np.uint8)\n        seg = cv2.cvtColor(seg, cv2.COLOR_GRAY2BGR)\n        seg = seg * color(classes[i])\n        seg = seg.astype(np.uint8)\n        image = cv2.add(image, seg)\n    return image\n\ndef setup_model(args):\n    model_path = args.model_path\n    if model_path.endswith(\'.pt\') or model_path.endswith(\'.torchscript\'):\n        platform = \'pytorch\'\n        from py_utils.pytorch_executor import Torch_model_container\n        model = Torch_model_container(args.model_path)\n    elif model_path.endswith(\'.rknn\'):\n        platform = \'rknn\'\n        from py_utils.rknn_executor import RKNN_model_container \n        model = RKNN_model_container(args.model_path, args.target, args.device_id)\n    elif model_path.endswith(\'onnx\'):\n        platform = \'onnx\'\n        from py_utils.onnx_executor import ONNX_model_container\n        model = ONNX_model_container(args.model_path)\n    else:\n        assert False, "{} is not rknn/pytorch/onnx model".format(model_path)\n    print(\'Model-{} is {} model, starting val\'.format(model_path, platform))\n    return model, platform\n\ndef img_check(path):\n    img_type = [\'.jpg\', \'.jpeg\', \'.png\', \'.bmp\']\n    for _type in img_type:\n        if path.endswith(_type) or path.endswith(_type.upper()):\n            return True\n    return False\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'YOLOv8-Seg Real-time Demo\')\n    parser.add_argument(\'--model_path\', type=str, required=True,\n                        help=\'model path, could be .pt or .rknn file\')\n    parser.add_argument(\'--target\', type=str, default=\'rk3566\',\n                        help=\'target RKNPU platform\')\n    parser.add_argument(\'--device_id\', type=str, default=None,\n                        help=\'device id\')\n    args = parser.parse_args()\n\n    # ---------- 1. \u521d\u59cb\u5316\u6a21\u578b ----------\n    model, platform = setup_model(args)\n    print(\'Model ready.\')\n\n    # ---------- 2. \u6253\u5f00\u6444\u50cf\u5934 ----------\n    cap = cv2.VideoCapture(0)\n    if not cap.isOpened():\n        print(\'Cannot open camera.\')\n        exit(-1)\n\n    co_helper = COCO_test_helper(enable_letter_box=True)\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # ---------- 3. \u9884\u5904\u7406 ----------\n        img = co_helper.letter_box(frame.copy(), IMG_SIZE, pad_color=(114, 114, 114))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        if platform in [\'pytorch\', \'onnx\']:\n            input_data = img.transpose(2, 0, 1).astype(np.float32) / 255.\n            input_data = np.expand_dims(input_data, 0)\n        else:\n            input_data = img\n\n        # ---------- 4. \u63a8\u7406 ----------\n        outputs = model.run([input_data])\n        boxes, classes, scores, seg_img = post_process(outputs)\n\n        # ---------- 5. \u53ef\u89c6\u5316 ----------\n        vis = frame.copy()\n        if boxes is not None:\n            real_boxes = co_helper.get_real_box(boxes)\n            real_segs  = co_helper.get_real_seg(seg_img)\n            vis = merge_seg(vis, real_segs, classes)\n            draw(vis, real_boxes, scores, classes)\n\n        cv2.imshow(\'YOLOv8-Seg\', vis)\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n\n    # ---------- 6. \u6e05\u7406 ----------\n    cap.release()\n    cv2.destroyAllWindows()\n    model.release()\n'})}),"\n",(0,t.jsx)(n.p,{children:"2.\u8fd0\u884c\u5982\u4e0b\u547d\u4ee4\u6267\u884c\u7a0b\u5e8f\uff1a"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"python3 yolov8_seg_video.py --model_path ../model/yolov8_seg.rknn --target rk3576\n"})})]})}function d(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},748:(e,n,o)=>{o.d(n,{Z:()=>t});const t=o.p+"assets/images/image-20250819161239333-963f39ec22b7396ff017422901a46eea.png"},1151:(e,n,o)=>{o.d(n,{Z:()=>i,a:()=>a});var t=o(7294);const s={},r=t.createContext(s);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);