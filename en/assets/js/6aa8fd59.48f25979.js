"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[421],{953:(n,e,o)=>{o.r(e),o.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var t=o(5893),r=o(1151);const s={sidebar_position:4},a="\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",i={id:"DshanPi-A1/part3/part3-3/03-2-4_TargetDetectionModelDep",title:"\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",description:"\u53c2\u8003\u8d44\u6599\uff1a",source:"@site/docs/DshanPi-A1/part3/part3-3/03-2-4_TargetDetectionModelDep.md",sourceDirName:"DshanPi-A1/part3/part3-3",slug:"/DshanPi-A1/part3/part3-3/03-2-4_TargetDetectionModelDep",permalink:"/en/docs/DshanPi-A1/part3/part3-3/03-2-4_TargetDetectionModelDep",draft:!1,unlisted:!1,editUrl:"https://github.com/100askTeam/linuxboard-docs/tree/main/docs/DshanPi-A1/part3/part3-3/03-2-4_TargetDetectionModelDep.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"dshanpia1Sidebar",previous:{title:"\u4eba\u5f62\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",permalink:"/en/docs/DshanPi-A1/part3/part3-3/03-2-3_HumanShapeDetectionModelDep"},next:{title:"\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u90e8\u7f72",permalink:"/en/docs/DshanPi-A1/part3/part3-3/03-2-5_InstanceSegmentationModelDep"}},l={},c=[{value:"1.\u83b7\u53d6\u539f\u59cb\u6a21\u578b",id:"1\u83b7\u53d6\u539f\u59cb\u6a21\u578b",level:2},{value:"2.\u6a21\u578b\u8f6c\u6362",id:"2\u6a21\u578b\u8f6c\u6362",level:2},{value:"3.\u6a21\u578b\u63a8\u7406",id:"3\u6a21\u578b\u63a8\u7406",level:2},{value:"4.\u89c6\u9891\u6d41\u63a8\u7406",id:"4\u89c6\u9891\u6d41\u63a8\u7406",level:2}];function d(n){const e={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.a)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h1,{id:"\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72",children:"\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72"}),"\n",(0,t.jsx)(e.p,{children:"\u53c2\u8003\u8d44\u6599\uff1a"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["\u6a21\u578b\u4ed3\u5e93\uff1a",(0,t.jsx)(e.a,{href:"https://github.com/airockchip/ultralytics_yolov8",children:"https://github.com/airockchip/ultralytics_yolov8"})]}),"\n",(0,t.jsxs)(e.li,{children:["\u6a21\u578b\u6e90\u7801\u8d44\u6599\u5305\uff08\u56fd\u5185\u7528\u6237\u63a8\u8350\uff09\uff1a",(0,t.jsx)(e.a,{href:"https://dl.100ask.net/Hardware/MPU/RK3576-DshanPi-A1/utils/ultralytics_yolov8.zip",children:"https://dl.100ask.net/Hardware/MPU/RK3576-DshanPi-A1/utils/ultralytics_yolov8.zip"})]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"1\u83b7\u53d6\u539f\u59cb\u6a21\u578b",children:"1.\u83b7\u53d6\u539f\u59cb\u6a21\u578b"}),"\n",(0,t.jsx)(e.p,{children:"1.\u8fdb\u5165\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4ed3\u5e93\uff1a"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"git clone https://github.com/airockchip/ultralytics_yolov8\ncd ultralytics_yolov8\n"})}),"\n",(0,t.jsx)(e.p,{children:"2.\u4f7f\u7528conda\u521b\u5efa\u73af\u5883"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"conda create -name yolov8 python=3.9\nconda activate yolov8\n"})}),"\n",(0,t.jsx)(e.p,{children:"3.\u5b89\u88c5yolov8\u76f8\u5173\u4f9d\u8d56"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"pip3 install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 ultralytics==8.3.31 onnx==1.17.0 onnxruntime==1.8.0 onnxsim==0.4.36\n"})}),"\n",(0,t.jsx)(e.p,{children:"4.\u5bfc\u51faONNX\u6a21\u578b"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"python ultralytics/engine/exporter.py\n"})}),"\n",(0,t.jsxs)(e.p,{children:["\u5982\u679c\u65e0\u6cd5\u4e0b\u8f7d\u6a21\u578b\u53ef\u76f4\u63a5\u8bbf\u95ee",(0,t.jsx)(e.a,{href:"https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt",children:"yolov8n.pt"}),"\uff0c\u4e0b\u8f7d\u540e\u653e\u5728",(0,t.jsx)(e.code,{children:"ultralytics_yolov8"}),"\u76ee\u5f55\u4e0b\uff0c\u518d\u6b21\u6267\u884c\u3002"]}),"\n",(0,t.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"(yolov8) baiwen@dshanpi-a1:~/ultralytics_yolov8$ python ultralytics/engine/exporter.py\nUltralytics YOLOv8.2.82 \ud83d\ude80 Python-3.9.23 torch-2.4.1 CPU (Cortex-A53)\nYOLOv8n summary (fused): 168 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n\nPyTorch: starting from 'yolov8n.pt' with input shape (16, 3, 640, 640) BCHW and output shape(s) ((16, 64, 80, 80), (16, 80, 80, 80), (16, 1, 80, 80), (16, 64, 40, 40), (16, 80, 40, 40), (16, 1, 40, 40), (16, 64, 20, 20), (16, 80, 20, 20), (16, 1, 20, 20)) (6.2 MB)\n\nRKNN: starting export with torch 2.4.1...\n\nRKNN: feed yolov8n.onnx to RKNN-Toolkit or RKNN-Toolkit2 to generate RKNN model.\nRefer https://github.com/airockchip/rknn_model_zoo/tree/main/models/CV/object_detection/yolo\nRKNN: export success \u2705 2.8s, saved as 'yolov8n.onnx' (12.1 MB)\n\nExport complete (24.8s)\nResults saved to /home/baiwen/ultralytics_yolov8\nPredict:         yolo predict task=detect model=yolov8n.onnx imgsz=640\nValidate:        yolo val task=detect model=yolov8n.onnx imgsz=640 data=coco.yaml\nVisualize:       https://netron.app\n"})}),"\n",(0,t.jsxs)(e.p,{children:["\u6267\u884c\u5b8c\u6210\u540e\u53ef\u4ee5\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u770b\u5230ONNX\u6a21\u578b\u6587\u4ef6",(0,t.jsx)(e.code,{children:"yolov8n.onnx"}),"\u3002"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"(yolov8) baiwen@dshanpi-a1:~/ultralytics_yolov8$ ls\nCITATION.cff     docker  examples  mkdocs.yml      README.md        RKOPT_README.md        tests        ultralytics.egg-info  yolov8n.pt\nCONTRIBUTING.md  docs    LICENSE   pyproject.toml  README.zh-CN.md  RKOPT_README.zh-CN.md  ultralytics  yolov8n.onnx\n"})}),"\n",(0,t.jsx)(e.p,{children:"\u5c06\u5bfc\u51fa\u7684ONNX\u6a21\u578b\u62f7\u8d1d\u81f3yolov8\u6a21\u578b\u76ee\u5f55\u3002"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"cp yolov8n.onnx ~/Projects/rknn_model_zoo/examples/yolov8/model\n"})}),"\n",(0,t.jsx)(e.h2,{id:"2\u6a21\u578b\u8f6c\u6362",children:"2.\u6a21\u578b\u8f6c\u6362"}),"\n",(0,t.jsxs)(e.p,{children:["1.\u4f7f\u7528Conda\u6fc0\u6d3b",(0,t.jsx)(e.code,{children:"rknn-toolkit2"}),"\u73af\u5883"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"conda activate rknn-toolkit2\n"})}),"\n",(0,t.jsx)(e.p,{children:"2.\u8fdb\u5165yolov8\u6a21\u578b\u8f6c\u6362\u76ee\u5f55"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"cd ~/Projects/rknn_model_zoo/examples/yolov8/python\n"})}),"\n",(0,t.jsx)(e.p,{children:"3.\u6267\u884c\u6a21\u578b\u8f6c\u6362"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"python3 convert.py ../model/yolov8n.onnx rk3576\n"})}),"\n",(0,t.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"\n(rknn-toolkit2) baiwen@dshanpi-a1:~/Projects/rknn_model_zoo/examples/yolov8/python$ python3 convert.py ../model/yolov8n.onnx rk3576\nI rknn-toolkit2 version: 2.3.2\n--\x3e Config model\ndone\n--\x3e Loading model\nI Loading : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 126/126 [00:00<00:00, 9216.48it/s]\ndone\n--\x3e Building model\nI OpFusing 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00<00:00, 173.69it/s]\nI OpFusing 1 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01<00:00, 99.72it/s]\nI OpFusing 0 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01<00:00, 84.03it/s]\nI OpFusing 1 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01<00:00, 74.45it/s]\nI OpFusing 2 : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:03<00:00, 26.99it/s]\nW build: found outlier value, this may affect quantization accuracy\n                        const name                        abs_mean    abs_std     outlier value\n                        model.0.conv.weight               2.44        2.47        -17.494\n                        model.22.cv3.2.1.conv.weight      0.09        0.14        -10.215\n                        model.22.cv3.1.1.conv.weight      0.12        0.19        13.361, 13.317\n                        model.22.cv3.0.1.conv.weight      0.18        0.20        -11.216\nI GraphPreparing : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 161/161 [00:00<00:00, 854.69it/s]\nI Quantizating : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 161/161 [00:32<00:00,  4.91it/s]\nW build: The default input dtype of 'images' is changed from 'float32' to 'int8' in rknn model for performance!\n                       Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '318' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of 'onnx::ReduceSum_326' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '331' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '338' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of 'onnx::ReduceSum_346' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '350' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '357' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of 'onnx::ReduceSum_365' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nW build: The default output dtype of '369' is changed from 'float32' to 'int8' in rknn model for performance!\n                      Please take care of this change when deploy rknn model with Runtime API!\nI rknn building ...\nI rknn building done.\ndone\n--\x3e Export rknn model\ndone\n"})}),"\n",(0,t.jsx)(e.p,{children:"\u53ef\u4ee5\u770b\u5230\u8f6c\u6362\u5b8c\u6210\u540e\u5728model\u76ee\u5f55\u4e0b\u770b\u5230\u7aef\u4fa7\u7684RKNN\u6a21\u578b\u3002"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"(rknn-toolkit2) baiwen@dshanpi-a1:~/Projects/rknn_model_zoo/examples/yolov8/python$ ls ../model\nbus.jpg  coco_80_labels_list.txt  dataset.txt  download_model.sh  yolov8n.onnx  yolov8.rknn\n"})}),"\n",(0,t.jsx)(e.h2,{id:"3\u6a21\u578b\u63a8\u7406",children:"3.\u6a21\u578b\u63a8\u7406"}),"\n",(0,t.jsx)(e.p,{children:"\u6267\u884c\u63a8\u7406\u6d4b\u8bd5\u4ee3\u7801\uff1a"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"python3 yolov8.py --model_path ../model/yolov8.rknn --target rk3576 --img_show\n"})}),"\n",(0,t.jsx)(e.p,{children:"\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"(rknn-toolkit2) baiwen@dshanpi-a1:~/Projects/rknn_model_zoo/examples/yolov8/python$ python3 yolov8.py --model_path ../model/yolov8.rknn --target rk3576\nI rknn-toolkit2 version: 2.3.2\n--\x3e Init runtime environment\nI target set by user is: rk3576\ndone\nModel-../model/yolov8.rknn is rknn model, starting val\nW inference: The 'data_format' is not set, and its default value is 'nhwc'!\n"})}),"\n",(0,t.jsx)(e.p,{children:"\u8fd0\u884c\u540e\u4f1a\u5f39\u51fa\u4e0b\u56fe\u6240\u793a\u7684\u68c0\u6d4b\u7ed3\u679c\u56fe\uff1a"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{alt:"image-20250819151219428",src:o(871).Z+"",width:"640",height:"640"})}),"\n",(0,t.jsx)(e.h2,{id:"4\u89c6\u9891\u6d41\u63a8\u7406",children:"4.\u89c6\u9891\u6d41\u63a8\u7406"}),"\n",(0,t.jsxs)(e.blockquote,{children:["\n",(0,t.jsx)(e.p,{children:"\u5f00\u59cb\u524d\u8bf7\u6ce8\u610f\uff0c\u8bf7\u52a1\u5fc5\u63a5\u5165USB\u6444\u50cf\u5934\uff0c\u5e76\u786e\u8ba4/dev/\u76ee\u5f55\u4e0b\u5b58\u5728video0\u8bbe\u5907\u8282\u70b9\uff01\uff01\uff01"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:["1.\u65b0\u5efa\u7a0b\u5e8f\u6587\u4ef6",(0,t.jsx)(e.code,{children:"yolov8_video.py.py"}),",\u586b\u5165\u4e00\u4e0b\u5185\u5bb9\uff1a"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:'import os\nimport cv2\nimport sys\nimport argparse\n\n# add path\nrealpath = os.path.abspath(__file__)\n_sep = os.path.sep\nrealpath = realpath.split(_sep)\nsys.path.append(os.path.join(realpath[0]+_sep, *realpath[1:realpath.index(\'rknn_model_zoo\')+1]))\n\nfrom py_utils.coco_utils import COCO_test_helper\nimport numpy as np\n\n\nOBJ_THRESH = 0.25\nNMS_THRESH = 0.45\n\n# The follew two param is for map test\n# OBJ_THRESH = 0.001\n# NMS_THRESH = 0.65\n\nIMG_SIZE = (640, 640)  # (width, height), such as (1280, 736)\n\nCLASSES = ("person", "bicycle", "car","motorbike ","aeroplane ","bus ","train","truck ","boat","traffic light",\n           "fire hydrant","stop sign ","parking meter","bench","bird","cat","dog ","horse ","sheep","cow","elephant",\n           "bear","zebra ","giraffe","backpack","umbrella","handbag","tie","suitcase","frisbee","skis","snowboard","sports ball","kite",\n           "baseball bat","baseball glove","skateboard","surfboard","tennis racket","bottle","wine glass","cup","fork","knife ",\n           "spoon","bowl","banana","apple","sandwich","orange","broccoli","carrot","hot dog","pizza ","donut","cake","chair","sofa",\n           "pottedplant","bed","diningtable","toilet ","tvmonitor","laptop\t","mouse\t","remote ","keyboard ","cell phone","microwave ",\n           "oven ","toaster","sink","refrigerator ","book","clock","vase","scissors ","teddy bear ","hair drier", "toothbrush ")\n\ncoco_id_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,\n         35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n         64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n\n\ndef filter_boxes(boxes, box_confidences, box_class_probs):\n    """Filter boxes with object threshold.\n    """\n    box_confidences = box_confidences.reshape(-1)\n    candidate, class_num = box_class_probs.shape\n\n    class_max_score = np.max(box_class_probs, axis=-1)\n    classes = np.argmax(box_class_probs, axis=-1)\n\n    _class_pos = np.where(class_max_score* box_confidences >= OBJ_THRESH)\n    scores = (class_max_score* box_confidences)[_class_pos]\n\n    boxes = boxes[_class_pos]\n    classes = classes[_class_pos]\n\n    return boxes, classes, scores\n\ndef nms_boxes(boxes, scores):\n    """Suppress non-maximal boxes.\n    # Returns\n        keep: ndarray, index of effective boxes.\n    """\n    x = boxes[:, 0]\n    y = boxes[:, 1]\n    w = boxes[:, 2] - boxes[:, 0]\n    h = boxes[:, 3] - boxes[:, 1]\n\n    areas = w * h\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n\n        xx1 = np.maximum(x[i], x[order[1:]])\n        yy1 = np.maximum(y[i], y[order[1:]])\n        xx2 = np.minimum(x[i] + w[i], x[order[1:]] + w[order[1:]])\n        yy2 = np.minimum(y[i] + h[i], y[order[1:]] + h[order[1:]])\n\n        w1 = np.maximum(0.0, xx2 - xx1 + 0.00001)\n        h1 = np.maximum(0.0, yy2 - yy1 + 0.00001)\n        inter = w1 * h1\n\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n        inds = np.where(ovr <= NMS_THRESH)[0]\n        order = order[inds + 1]\n    keep = np.array(keep)\n    return keep\n\ndef dfl(position):\n    # Distribution Focal Loss (DFL)\n    import torch\n    x = torch.tensor(position)\n    n,c,h,w = x.shape\n    p_num = 4\n    mc = c//p_num\n    y = x.reshape(n,p_num,mc,h,w)\n    y = y.softmax(2)\n    acc_metrix = torch.tensor(range(mc)).float().reshape(1,1,mc,1,1)\n    y = (y*acc_metrix).sum(2)\n    return y.numpy()\n\n\ndef box_process(position):\n    grid_h, grid_w = position.shape[2:4]\n    col, row = np.meshgrid(np.arange(0, grid_w), np.arange(0, grid_h))\n    col = col.reshape(1, 1, grid_h, grid_w)\n    row = row.reshape(1, 1, grid_h, grid_w)\n    grid = np.concatenate((col, row), axis=1)\n    stride = np.array([IMG_SIZE[1]//grid_h, IMG_SIZE[0]//grid_w]).reshape(1,2,1,1)\n\n    position = dfl(position)\n    box_xy  = grid +0.5 -position[:,0:2,:,:]\n    box_xy2 = grid +0.5 +position[:,2:4,:,:]\n    xyxy = np.concatenate((box_xy*stride, box_xy2*stride), axis=1)\n\n    return xyxy\n\ndef post_process(input_data):\n    boxes, scores, classes_conf = [], [], []\n    defualt_branch=3\n    pair_per_branch = len(input_data)//defualt_branch\n    # Python \u5ffd\u7565 score_sum \u8f93\u51fa\n    for i in range(defualt_branch):\n        boxes.append(box_process(input_data[pair_per_branch*i]))\n        classes_conf.append(input_data[pair_per_branch*i+1])\n        scores.append(np.ones_like(input_data[pair_per_branch*i+1][:,:1,:,:], dtype=np.float32))\n\n    def sp_flatten(_in):\n        ch = _in.shape[1]\n        _in = _in.transpose(0,2,3,1)\n        return _in.reshape(-1, ch)\n\n    boxes = [sp_flatten(_v) for _v in boxes]\n    classes_conf = [sp_flatten(_v) for _v in classes_conf]\n    scores = [sp_flatten(_v) for _v in scores]\n\n    boxes = np.concatenate(boxes)\n    classes_conf = np.concatenate(classes_conf)\n    scores = np.concatenate(scores)\n\n    # filter according to threshold\n    boxes, classes, scores = filter_boxes(boxes, scores, classes_conf)\n\n    # nms\n    nboxes, nclasses, nscores = [], [], []\n    for c in set(classes):\n        inds = np.where(classes == c)\n        b = boxes[inds]\n        c = classes[inds]\n        s = scores[inds]\n        keep = nms_boxes(b, s)\n\n        if len(keep) != 0:\n            nboxes.append(b[keep])\n            nclasses.append(c[keep])\n            nscores.append(s[keep])\n\n    if not nclasses and not nscores:\n        return None, None, None\n\n    boxes = np.concatenate(nboxes)\n    classes = np.concatenate(nclasses)\n    scores = np.concatenate(nscores)\n\n    return boxes, classes, scores\n\n\ndef draw(image, boxes, scores, classes):\n    for box, score, cl in zip(boxes, scores, classes):\n        top, left, right, bottom = [int(_b) for _b in box]\n        print("%s @ (%d %d %d %d) %.3f" % (CLASSES[cl], top, left, right, bottom, score))\n        cv2.rectangle(image, (top, left), (right, bottom), (255, 0, 0), 2)\n        cv2.putText(image, \'{0} {1:.2f}\'.format(CLASSES[cl], score),\n                    (top, left - 6), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n\ndef setup_model(args):\n    model_path = args.model_path\n    if model_path.endswith(\'.pt\') or model_path.endswith(\'.torchscript\'):\n        platform = \'pytorch\'\n        from py_utils.pytorch_executor import Torch_model_container\n        model = Torch_model_container(args.model_path)\n    elif model_path.endswith(\'.rknn\'):\n        platform = \'rknn\'\n        from py_utils.rknn_executor import RKNN_model_container \n        model = RKNN_model_container(args.model_path, args.target, args.device_id)\n    elif model_path.endswith(\'onnx\'):\n        platform = \'onnx\'\n        from py_utils.onnx_executor import ONNX_model_container\n        model = ONNX_model_container(args.model_path)\n    else:\n        assert False, "{} is not rknn/pytorch/onnx model".format(model_path)\n    print(\'Model-{} is {} model, starting val\'.format(model_path, platform))\n    return model, platform\n\ndef img_check(path):\n    img_type = [\'.jpg\', \'.jpeg\', \'.png\', \'.bmp\']\n    for _type in img_type:\n        if path.endswith(_type) or path.endswith(_type.upper()):\n            return True\n    return False\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'YOLOv8-Seg Real-time Demo\')\n    parser.add_argument(\'--model_path\', type=str, required=True,\n                        help=\'model path, could be .pt or .rknn file\')\n    parser.add_argument(\'--target\', type=str, default=\'rk3566\',\n                        help=\'target RKNPU platform\')\n    parser.add_argument(\'--device_id\', type=str, default=None,\n                        help=\'device id\')\n    args = parser.parse_args()\n\n    # 1. \u521d\u59cb\u5316\u6a21\u578b\n    model, platform = setup_model(args)\n    print(\'Model ready.\')\n\n    # 2. \u6253\u5f00\u6444\u50cf\u5934\n    cap = cv2.VideoCapture(0)\n    if not cap.isOpened():\n        print(\'Cannot open camera.\')\n        exit(-1)\n\n    # 3. \u5b9e\u65f6\u5faa\u73af\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        h0, w0 = frame.shape[:2]\n\n        # 3-1 LetterBox \u9884\u5904\u7406\n        co_helper = COCO_test_helper(enable_letter_box=True)\n        img = co_helper.letter_box(frame.copy(), IMG_SIZE, pad_color=(0, 0, 0))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # 3-2 \u6784\u9020\u8f93\u5165\n        if platform in [\'pytorch\', \'onnx\']:\n            input_data = img.transpose(2, 0, 1).astype(np.float32) / 255.\n            input_data = np.expand_dims(input_data, 0)\n        else:\n            input_data = img\n\n        # 3-3 \u63a8\u7406\n        outputs = model.run([input_data])\n        boxes, classes, scores = post_process(outputs)\n\n        # 3-4 \u753b\u6846\n        vis = frame.copy()\n        if boxes is not None:\n            boxes_real = co_helper.get_real_box(boxes)\n            draw(vis, boxes_real, scores, classes)\n\n        # 3-5 \u5b9e\u65f6\u663e\u793a\n        cv2.imshow(\'YOLOv8\', vis)\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n    model.release()\n'})}),"\n",(0,t.jsx)(e.p,{children:"\u8fd0\u884c\u5982\u4e0b\u547d\u4ee4\u63a8\u7406\uff1a"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"python3 yolov8_video.py --model_path ../model/yolov8.rknn --target rk3576\n"})})]})}function p(n={}){const{wrapper:e}={...(0,r.a)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},871:(n,e,o)=>{o.d(e,{Z:()=>t});const t=o.p+"assets/images/image-20250819151219428-fe3b439bb2f200e09b17b391927791b1.png"},1151:(n,e,o)=>{o.d(e,{Z:()=>i,a:()=>a});var t=o(7294);const r={},s=t.createContext(r);function a(n){const e=t.useContext(s);return t.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:a(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);